{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyMLGOQB12yuFBmVVbOC+FpM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShabnaIlmi/Data-Science-Group-Project/blob/recipe-risk-analyzer/DSGP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "!pip install imblearn\n",
        "!pip install xgboost\n",
        "!pip install joblib\n"
      ],
      "metadata": {
        "id": "Dxv5RWe_xE-u",
        "outputId": "da2e6c78-340a-49ee-e687-9b2740275b8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.11/dist-packages (0.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (from imblearn) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.13.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "file_path = \"chemical_recipe_dataset.csv\"  # Replace with the actual file path if necessary\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vessNO4bxFs0",
        "outputId": "cb5ca246-73bc-46ad-b807-3ede5a656bef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Recipe ID                                     Chemical Names  \\\n",
            "0     R0001  Acetone + Hydrogen Peroxide + Sulfuric Acid + ...   \n",
            "1     R0002              Charcoal + Potassium Nitrate + Sulfur   \n",
            "2     R0003                         Hydrogen Sulfide + Ammonia   \n",
            "3     R0004                          Sulfur + Ammonium Nitrate   \n",
            "4     R0005    Hydrogen Sulfide + Ammonia + Methane + Chlorine   \n",
            "\n",
            "                  Quantities   Category  \\\n",
            "0  485g + 398g + 275g + 197g  Explosive   \n",
            "1          465g + 134g + 72g  Explosive   \n",
            "2                272g + 358g  Corrosive   \n",
            "3                297g + 304g   Unstable   \n",
            "4   74g + 376g + 285g + 199g  Corrosive   \n",
            "\n",
            "                                 Potential Reaction Risk Level  \n",
            "0   Explosion risk when exposed to heat or friction        Low  \n",
            "1   Explosion risk when exposed to heat or friction     Medium  \n",
            "2                    Causes severe burns on contact     Medium  \n",
            "3  May decompose violently under certain conditions       High  \n",
            "4                    Causes severe burns on contact     Medium  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the 'Risk Level' (target variable)\n",
        "label_encoder = LabelEncoder()\n",
        "data['Risk Level Encoded'] = label_encoder.fit_transform(data['Risk Level'])\n",
        "\n",
        "# One-hot encode the 'Chemical Names' and 'Category'\n",
        "chemical_dummies = data['Chemical Names'].str.get_dummies(sep=' + ')\n",
        "category_dummies = data['Category'].str.get_dummies(sep=', ')\n",
        "\n",
        "# Extract numerical values from 'Quantities' (sum the grams for simplicity)\n",
        "data['Total Quantity (g)'] = data['Quantities'].str.extractall(r'(\\d+)').astype(int).groupby(level=0).sum()\n",
        "\n",
        "# Combine all features\n",
        "X = pd.concat([chemical_dummies, category_dummies, data['Total Quantity (g)']], axis=1)\n",
        "y = data['Risk Level Encoded']\n"
      ],
      "metadata": {
        "id": "cDAhdjzExKiF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data first\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Apply SMOTE only to training data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(f\"Original class distribution: {dict(zip(*np.unique(y_train, return_counts=True)))}\")\n",
        "print(f\"Balanced class distribution: {dict(zip(*np.unique(y_train_balanced, return_counts=True)))}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfVL1zx9yini",
        "outputId": "b4833c3c-7525-4e84-a251-b4145700103a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original class distribution: {0: 272, 1: 270, 2: 258}\n",
            "Balanced class distribution: {0: 272, 1: 272, 2: 272}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "yJWc1LSSxOA1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Define the model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_distributions = {\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'max_depth': [10, 20, 30, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2', None]\n",
        "}\n",
        "\n",
        "# Perform RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=rf, param_distributions=param_distributions,\n",
        "    n_iter=50, cv=3, scoring='accuracy', random_state=42, n_jobs=-1\n",
        ")\n",
        "random_search.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "# Best parameters and score\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", random_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZggJaOTO0Y9C",
        "outputId": "15595986-12a6-4215-d1c7-e497c57115a0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None, 'max_depth': 10}\n",
            "Best Cross-Validation Accuracy: 0.36642156862745096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Random Forest Classifier\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances and select top features\n",
        "# Use random_search.best_estimator_ to access the fitted model\n",
        "importances = random_search.best_estimator_.feature_importances_\n",
        "important_indices = np.argsort(importances)[-10:]  # Top 10 features\n",
        "X_selected = X.iloc[:, important_indices]\n",
        "\n",
        "print(\"Top Features:\", X.columns[important_indices])\n",
        "\n",
        "print(\"Model training complete.\")"
      ],
      "metadata": {
        "id": "QOPI8ZVdxQet",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d486ac4c-131e-4a2a-9c86-316f2d16dc65"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Features: Index([' + Chlorine + Hydrogen Sulfide + ',\n",
            "       ' + Chlorine + Hydrogen Sulfide + Ammonia + ',\n",
            "       ' + Chlorine + Hydrogen Sulfide + Ammonia + Methane + ',\n",
            "       ' + Chlorine + Hydrogen Sulfide + Methane + ', 'Flammable', 'Toxic',\n",
            "       'Corrosive', 'Explosive', 'Unstable', 'Total Quantity (g)'],\n",
            "      dtype='object')\n",
            "Model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "v4CMhlsCxSdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "941e66b4-592a-4cff-8cbd-977d5468e67b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[16 26 20]\n",
            " [23 18 30]\n",
            " [32 21 14]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.23      0.26      0.24        62\n",
            "           1       0.28      0.25      0.26        71\n",
            "           2       0.22      0.21      0.21        67\n",
            "\n",
            "    accuracy                           0.24       200\n",
            "   macro avg       0.24      0.24      0.24       200\n",
            "weighted avg       0.24      0.24      0.24       200\n",
            "\n",
            "\n",
            "Accuracy Score: 0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the trained model to a file\n",
        "joblib.dump(model, \"risk_prediction_model.pkl\")\n",
        "print(\"Model saved as 'risk_prediction_model.pkl'\")\n"
      ],
      "metadata": {
        "id": "Pk1BwUepxVeM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "956c70ce-b2f7-4a19-8305-d451f1bba404"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as 'risk_prediction_model.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Predict risk for a new chemical combination\n",
        "new_data = pd.DataFrame({\n",
        "    \"Chemical Names\": [\"Ammonium Nitrate + Hydrogen Peroxide\"],\n",
        "    \"Category\": [\"Explosive, Toxic Liquid\"],\n",
        "    \"Quantities\": [\"200g + 100g\"]\n",
        "})\n",
        "\n",
        "# Preprocess the new data\n",
        "new_data['Total Quantity (g)'] = new_data['Quantities'].str.extractall(r'(\\d+)').astype(int).groupby(level=0).sum()\n",
        "\n",
        "# Create dummy features matching training data\n",
        "chemical_features = pd.DataFrame(columns=chemical_dummies.columns)\n",
        "category_features = pd.DataFrame(columns=category_dummies.columns)\n",
        "\n",
        "# Fill in the matching dummy values\n",
        "for col in chemical_features.columns:\n",
        "    chemical_features.at[0, col] = 1 if col in new_data['Chemical Names'][0].split(\" + \") else 0\n",
        "\n",
        "for col in category_features.columns:\n",
        "    category_features.at[0, col] = 1 if col in new_data['Category'][0].split(\", \") else 0\n",
        "\n",
        "# Combine all features\n",
        "new_features = pd.concat([chemical_features, category_features], axis=1)\n",
        "new_features['Total Quantity (g)'] = new_data['Total Quantity (g)']\n",
        "\n",
        "# Fill missing columns with zeros to match training data\n",
        "new_features = new_features.reindex(columns=X.columns, fill_value=0)\n",
        "\n",
        "# Standardize\n",
        "new_features_scaled = scaler.transform(new_features)\n",
        "\n",
        "# Predict\n",
        "predicted_risk = model.predict(new_features_scaled)\n",
        "predicted_risk_label = label_encoder.inverse_transform(predicted_risk)\n",
        "\n",
        "print(\"Predicted Risk Level:\", predicted_risk_label[0])\n"
      ],
      "metadata": {
        "id": "qAWTudRIyCel",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09415178-1396-4067-f2e9-dd95ac6d5f0b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Risk Level: Low\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the trained scaler and label encoder\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n",
        "joblib.dump(label_encoder, \"label_encoder.pkl\")\n"
      ],
      "metadata": {
        "id": "nhOCzZsbzXE9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb4fcc80-a870-429b-bbfc-4b92a4cc48b6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['label_encoder.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "# Load the previously trained model and preprocessing objects\n",
        "import joblib\n",
        "model = joblib.load(\"risk_prediction_model.pkl\")  # Load your trained model\n",
        "scaler = joblib.load(\"scaler.pkl\")  # Assuming you saved the scaler\n",
        "label_encoder = joblib.load(\"label_encoder.pkl\")  # Assuming you saved the label encoder\n",
        "\n",
        "# Define function to process input chemicals and make prediction\n",
        "def predict_chemical_risk(chemical_names, quantities, categories):\n",
        "    # Creating the dataframe from the input\n",
        "    input_data = pd.DataFrame({\n",
        "        \"Chemical Names\": [chemical_names],\n",
        "        \"Quantities\": [quantities],\n",
        "        \"Category\": [categories]\n",
        "    })\n",
        "\n",
        "    # Extracting and summing the quantities\n",
        "    input_data['Total Quantity (g)'] = input_data['Quantities'].str.extractall(r'(\\d+)').astype(int).groupby(level=0).sum()\n",
        "\n",
        "    # Creating feature columns for one-hot encoding\n",
        "    chemical_features = pd.DataFrame(columns=chemical_dummies.columns)\n",
        "    category_features = pd.DataFrame(columns=category_dummies.columns)\n",
        "\n",
        "    # Fill the chemical features based on input data\n",
        "    for col in chemical_features.columns:\n",
        "        chemical_features.at[0, col] = 1 if col in chemical_names.split(\" + \") else 0\n",
        "\n",
        "    # Fill the category features based on input data\n",
        "    for col in category_features.columns:\n",
        "        category_features.at[0, col] = 1 if col in categories.split(\", \") else 0\n",
        "\n",
        "    # Combine the features\n",
        "    new_features = pd.concat([chemical_features, category_features], axis=1)\n",
        "    new_features['Total Quantity (g)'] = input_data['Total Quantity (g)']\n",
        "\n",
        "    # Fill any missing columns from the training data\n",
        "    new_features = new_features.reindex(columns=X.columns, fill_value=0)\n",
        "\n",
        "    # Standardize the new data using the same scaler\n",
        "    new_features_scaled = scaler.transform(new_features)\n",
        "\n",
        "    # Predict risk\n",
        "    predicted_risk = model.predict(new_features_scaled)\n",
        "    predicted_risk_label = label_encoder.inverse_transform(predicted_risk)\n",
        "\n",
        "    return predicted_risk_label[0]\n",
        "\n",
        "# Example usage\n",
        "chemical_input = input(\"Enter chemical names (separate by ' + '): \")\n",
        "quantity_input = input(\"Enter quantities (separate by ' + '): \")\n",
        "category_input = input(\"Enter categories (separate by ', '): \")\n",
        "\n",
        "predicted_risk = predict_chemical_risk(chemical_input, quantity_input, category_input)\n",
        "\n",
        "print(\"Predicted Risk Level:\", predicted_risk)\n"
      ],
      "metadata": {
        "id": "JZn_ahc0y0PV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a01dc90-7e30-4f1c-a853-78081b31767b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter chemical names (separate by ' + '): salt + water\n",
            "Enter quantities (separate by ' + '): 30g+800g\n",
            "Enter categories (separate by ', '): liquid\n",
            "Predicted Risk Level: Medium\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_balanced)  # Scale only for XGBoost\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "iowdo1mead9t",
        "outputId": "ffa5bcb1-54c9-4352-ba36-ac5b058bb2b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "xgb_model = XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "xgb_model.fit(X_train_scaled, y_train_balanced)  # Use scaled data for XGBoost\n",
        "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "BVCIUvGUszt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_distributions = {\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'max_depth': [10, 20, 30, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2', None]\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    RandomForestClassifier(random_state=42),\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=50, cv=5, scoring='accuracy', n_jobs=-1, random_state=42\n",
        ")\n",
        "random_search.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "best_rf = random_search.best_estimator_  # Use best RF model\n",
        "print(\"Best Random Forest Parameters:\", random_search.best_params_)\n"
      ],
      "metadata": {
        "id": "izYdKIcCwHjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_model.set_params(verbosity=0)\n"
      ],
      "metadata": {
        "id": "RFqvwXg_eqjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "print(sklearn.__version__)  # Ensure it's 1.0 or newer\n",
        "!pip install -U scikit-learn\n"
      ],
      "metadata": {
        "id": "RkQXyQEPccHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "xgb_model = XGBClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "rf_model.fit(X_train_balanced, y_train_balanced)\n",
        "xgb_model.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('rf', rf_model), ('xgb', xgb_model)],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "voting_clf.fit(X_train_balanced, y_train_balanced)\n"
      ],
      "metadata": {
        "id": "k3oZxWZid5ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Ensure XGBoost is compatible\n",
        "xgb_model.set_params(verbosity=0)\n",
        "\n",
        "# Create Voting Classifier\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('rf', best_rf),  # Best Random Forest model\n",
        "        ('xgb', xgb_model)  # XGBoost\n",
        "    ],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "# Fit the classifier\n",
        "voting_clf.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_voting = voting_clf.predict(X_test)\n"
      ],
      "metadata": {
        "id": "B2JIMseia4_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, best_rf.predict(X_test)))\n",
        "print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
        "print(\"Voting Classifier Accuracy:\", accuracy_score(y_test, y_pred_voting))\n",
        "\n",
        "print(\"\\nRandom Forest Report:\\n\", classification_report(y_test, best_rf.predict(X_test)))\n",
        "print(\"\\nXGBoost Report:\\n\", classification_report(y_test, y_pred_xgb))\n",
        "print(\"\\nVoting Classifier Report:\\n\", classification_report(y_test, y_pred_voting))\n"
      ],
      "metadata": {
        "id": "vuT2PIdFbg5k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}