{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyOORTaoKkcDtag32YbDFz2e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShabnaIlmi/Data-Science-Group-Project/blob/recipe-risk-analyzer/DSGP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "!pip install imblearn\n",
        "!pip install xgboost\n",
        "!pip install joblib\n"
      ],
      "metadata": {
        "id": "Dxv5RWe_xE-u",
        "outputId": "544b13a6-89af-44b0-b293-f584f577c167",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.11/dist-packages (0.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (from imblearn) (0.13.0)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.26.4)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.13.1)\n",
            "Collecting scikit-learn<2,>=1.3.2 (from imbalanced-learn->imblearn)\n",
            "  Using cached scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
            "Using cached scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed scikit-learn-1.6.1\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.25.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.13.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "file_path = \"chemical_recipe_dataset.csv\"  # Replace with the actual file path if necessary\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vessNO4bxFs0",
        "outputId": "cb3e56d3-2b3e-4d9b-9a55-c340bf16ddfd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Recipe ID                                     Chemical Names  \\\n",
            "0     R0001  Acetone + Hydrogen Peroxide + Sulfuric Acid + ...   \n",
            "1     R0002              Charcoal + Potassium Nitrate + Sulfur   \n",
            "2     R0003                         Hydrogen Sulfide + Ammonia   \n",
            "3     R0004                          Sulfur + Ammonium Nitrate   \n",
            "4     R0005    Hydrogen Sulfide + Ammonia + Methane + Chlorine   \n",
            "\n",
            "                  Quantities   Category  \\\n",
            "0  485g + 398g + 275g + 197g  Explosive   \n",
            "1          465g + 134g + 72g  Explosive   \n",
            "2                272g + 358g  Corrosive   \n",
            "3                297g + 304g   Unstable   \n",
            "4   74g + 376g + 285g + 199g  Corrosive   \n",
            "\n",
            "                                 Potential Reaction Risk Level  \n",
            "0   Explosion risk when exposed to heat or friction        Low  \n",
            "1   Explosion risk when exposed to heat or friction     Medium  \n",
            "2                    Causes severe burns on contact     Medium  \n",
            "3  May decompose violently under certain conditions       High  \n",
            "4                    Causes severe burns on contact     Medium  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the 'Risk Level' (target variable)\n",
        "label_encoder = LabelEncoder()\n",
        "data['Risk Level Encoded'] = label_encoder.fit_transform(data['Risk Level'])\n",
        "\n",
        "# One-hot encode the 'Chemical Names' and 'Category'\n",
        "chemical_dummies = data['Chemical Names'].str.get_dummies(sep=' + ')\n",
        "category_dummies = data['Category'].str.get_dummies(sep=', ')\n",
        "\n",
        "# Extract numerical values from 'Quantities' (sum the grams for simplicity)\n",
        "data['Total Quantity (g)'] = data['Quantities'].str.extractall(r'(\\d+)').astype(int).groupby(level=0).sum()\n",
        "\n",
        "# Combine all features\n",
        "X = pd.concat([chemical_dummies, category_dummies, data['Total Quantity (g)']], axis=1)\n",
        "y = data['Risk Level Encoded']\n"
      ],
      "metadata": {
        "id": "cDAhdjzExKiF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y scikit-learn imbalanced-learn\n",
        "!pip install -U scikit-learn imbalanced-learn\n"
      ],
      "metadata": {
        "id": "Ob4ZFhxA0HJe",
        "outputId": "0aa02e00-ace2-4a51-c7cd-108d5a82bef2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: scikit-learn 1.6.1\n",
            "Uninstalling scikit-learn-1.6.1:\n",
            "  Successfully uninstalled scikit-learn-1.6.1\n",
            "Found existing installation: imbalanced-learn 0.13.0\n",
            "Uninstalling imbalanced-learn-0.13.0:\n",
            "  Successfully uninstalled imbalanced-learn-0.13.0\n",
            "Collecting scikit-learn\n",
            "  Using cached scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting imbalanced-learn\n",
            "  Using cached imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Using cached scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "Using cached imbalanced_learn-0.13.0-py3-none-any.whl (238 kB)\n",
            "Installing collected packages: scikit-learn, imbalanced-learn\n",
            "Successfully installed imbalanced-learn-0.13.0 scikit-learn-1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming X and y are your features and labels\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply SMOTE to the training data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(f\"Original class distribution: {dict(zip(*np.unique(y_train, return_counts=True)))}\")\n",
        "print(f\"Balanced class distribution: {dict(zip(*np.unique(y_train_balanced, return_counts=True)))}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "xfVL1zx9yini",
        "outputId": "535a4d42-35e8-47eb-e59b-cd36ba080be9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'UnsetMetadataPassedError' from 'sklearn.exceptions' (/usr/local/lib/python3.11/dist-packages/sklearn/exceptions.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-cf0e6b2b7b02>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Assuming X and y are your features and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# process, as it may not be compiled yet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     from . import (\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mcombine\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mensemble\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/combine/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_smote_enn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTEENN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_smote_tomek\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTETomek\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/combine/_smote_enn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseOverSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOneToOneFeatureMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlabel_binarize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata_requests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMETHODS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulticlass\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_metadata_requests.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnsetMetadataPassedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_bunch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBunch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'UnsetMetadataPassedError' from 'sklearn.exceptions' (/usr/local/lib/python3.11/dist-packages/sklearn/exceptions.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "yJWc1LSSxOA1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Define the model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_distributions = {\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'max_depth': [10, 20, 30, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2', None]\n",
        "}\n",
        "\n",
        "# Perform RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=rf, param_distributions=param_distributions,\n",
        "    n_iter=50, cv=3, scoring='accuracy', random_state=42, n_jobs=-1\n",
        ")\n",
        "random_search.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "# Best parameters and score\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", random_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "ZggJaOTO0Y9C",
        "outputId": "f302d6df-3b2b-4c2a-aee6-fb98ca6180c5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train_balanced' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-6871aeffc116>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m )\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_balanced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_balanced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Best parameters and score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train_balanced' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Random Forest Classifier\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances and select top features\n",
        "# Use random_search.best_estimator_ to access the fitted model\n",
        "importances = random_search.best_estimator_.feature_importances_\n",
        "important_indices = np.argsort(importances)[-10:]  # Top 10 features\n",
        "X_selected = X.iloc[:, important_indices]\n",
        "\n",
        "print(\"Top Features:\", X.columns[important_indices])\n",
        "\n",
        "print(\"Model training complete.\")"
      ],
      "metadata": {
        "id": "QOPI8ZVdxQet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "v4CMhlsCxSdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the trained model to a file\n",
        "joblib.dump(model, \"risk_prediction_model.pkl\")\n",
        "print(\"Model saved as 'risk_prediction_model.pkl'\")\n"
      ],
      "metadata": {
        "id": "Pk1BwUepxVeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Predict risk for a new chemical combination\n",
        "new_data = pd.DataFrame({\n",
        "    \"Chemical Names\": [\"Ammonium Nitrate + Hydrogen Peroxide\"],\n",
        "    \"Category\": [\"Explosive, Toxic Liquid\"],\n",
        "    \"Quantities\": [\"200g + 100g\"]\n",
        "})\n",
        "\n",
        "# Preprocess the new data\n",
        "new_data['Total Quantity (g)'] = new_data['Quantities'].str.extractall(r'(\\d+)').astype(int).groupby(level=0).sum()\n",
        "\n",
        "# Create dummy features matching training data\n",
        "chemical_features = pd.DataFrame(columns=chemical_dummies.columns)\n",
        "category_features = pd.DataFrame(columns=category_dummies.columns)\n",
        "\n",
        "# Fill in the matching dummy values\n",
        "for col in chemical_features.columns:\n",
        "    chemical_features.at[0, col] = 1 if col in new_data['Chemical Names'][0].split(\" + \") else 0\n",
        "\n",
        "for col in category_features.columns:\n",
        "    category_features.at[0, col] = 1 if col in new_data['Category'][0].split(\", \") else 0\n",
        "\n",
        "# Combine all features\n",
        "new_features = pd.concat([chemical_features, category_features], axis=1)\n",
        "new_features['Total Quantity (g)'] = new_data['Total Quantity (g)']\n",
        "\n",
        "# Fill missing columns with zeros to match training data\n",
        "new_features = new_features.reindex(columns=X.columns, fill_value=0)\n",
        "\n",
        "# Standardize\n",
        "new_features_scaled = scaler.transform(new_features)\n",
        "\n",
        "# Predict\n",
        "predicted_risk = model.predict(new_features_scaled)\n",
        "predicted_risk_label = label_encoder.inverse_transform(predicted_risk)\n",
        "\n",
        "print(\"Predicted Risk Level:\", predicted_risk_label[0])\n"
      ],
      "metadata": {
        "id": "qAWTudRIyCel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the trained scaler and label encoder\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n",
        "joblib.dump(label_encoder, \"label_encoder.pkl\")\n"
      ],
      "metadata": {
        "id": "nhOCzZsbzXE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "# Load the previously trained model and preprocessing objects\n",
        "import joblib\n",
        "model = joblib.load(\"risk_prediction_model.pkl\")  # Load your trained model\n",
        "scaler = joblib.load(\"scaler.pkl\")  # Assuming you saved the scaler\n",
        "label_encoder = joblib.load(\"label_encoder.pkl\")  # Assuming you saved the label encoder\n",
        "\n",
        "# Define function to process input chemicals and make prediction\n",
        "def predict_chemical_risk(chemical_names, quantities, categories):\n",
        "    # Creating the dataframe from the input\n",
        "    input_data = pd.DataFrame({\n",
        "        \"Chemical Names\": [chemical_names],\n",
        "        \"Quantities\": [quantities],\n",
        "        \"Category\": [categories]\n",
        "    })\n",
        "\n",
        "    # Extracting and summing the quantities\n",
        "    input_data['Total Quantity (g)'] = input_data['Quantities'].str.extractall(r'(\\d+)').astype(int).groupby(level=0).sum()\n",
        "\n",
        "    # Creating feature columns for one-hot encoding\n",
        "    chemical_features = pd.DataFrame(columns=chemical_dummies.columns)\n",
        "    category_features = pd.DataFrame(columns=category_dummies.columns)\n",
        "\n",
        "    # Fill the chemical features based on input data\n",
        "    for col in chemical_features.columns:\n",
        "        chemical_features.at[0, col] = 1 if col in chemical_names.split(\" + \") else 0\n",
        "\n",
        "    # Fill the category features based on input data\n",
        "    for col in category_features.columns:\n",
        "        category_features.at[0, col] = 1 if col in categories.split(\", \") else 0\n",
        "\n",
        "    # Combine the features\n",
        "    new_features = pd.concat([chemical_features, category_features], axis=1)\n",
        "    new_features['Total Quantity (g)'] = input_data['Total Quantity (g)']\n",
        "\n",
        "    # Fill any missing columns from the training data\n",
        "    new_features = new_features.reindex(columns=X.columns, fill_value=0)\n",
        "\n",
        "    # Standardize the new data using the same scaler\n",
        "    new_features_scaled = scaler.transform(new_features)\n",
        "\n",
        "    # Predict risk\n",
        "    predicted_risk = model.predict(new_features_scaled)\n",
        "    predicted_risk_label = label_encoder.inverse_transform(predicted_risk)\n",
        "\n",
        "    return predicted_risk_label[0]\n",
        "\n",
        "# Example usage\n",
        "chemical_input = input(\"Enter chemical names (separate by ' + '): \")\n",
        "quantity_input = input(\"Enter quantities (separate by ' + '): \")\n",
        "category_input = input(\"Enter categories (separate by ', '): \")\n",
        "\n",
        "predicted_risk = predict_chemical_risk(chemical_input, quantity_input, category_input)\n",
        "\n",
        "print(\"Predicted Risk Level:\", predicted_risk)\n"
      ],
      "metadata": {
        "id": "JZn_ahc0y0PV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "xgb_model = XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "xgb_model.fit(X_train_balanced, y_train_balanced)\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "BVCIUvGUszt2",
        "outputId": "61782f93-265e-4d48-d68d-d340b58b966c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train_balanced' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-cebff9792e41>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mxgb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_balanced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_balanced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train_balanced' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_distributions = {\n",
        "    'n_estimators': [100, 200, 300, 500],\n",
        "    'max_depth': [None, 10, 20, 50],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 5],\n",
        "    'bootstrap': [True, False],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=RandomForestClassifier(random_state=42),\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=100,  # Larger n_iter for better tuning\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "random_search.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n"
      ],
      "metadata": {
        "id": "izYdKIcCwHjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier # Import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('rf', RandomForestClassifier(random_state=42)),\n",
        "        ('xgb', XGBClassifier(random_state=42))\n",
        "    ],\n",
        "    voting='soft'\n",
        ")\n",
        "voting_clf.fit(X_train_balanced, y_train_balanced)"
      ],
      "metadata": {
        "id": "JwtvKSarxmno"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}