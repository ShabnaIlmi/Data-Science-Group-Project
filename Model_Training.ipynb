{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShabnaIlmi/Data-Science-Group-Project/blob/Importer_Risk_Prediction/Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpJO0Ib3RGsE"
      },
      "outputs": [],
      "source": [
        "# Installing required libraries\n",
        "# !pip install --upgrade tensorflow\n",
        "# !pip install fancyimpute scikit-learn pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {
        "id": "8Q2eNv9ERdHW"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import IterativeImputer, KNNImputer\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "import warnings\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import KNNImputer\n",
        "import random\n",
        "import pickle\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQeCGQRxRggh"
      },
      "outputs": [],
      "source": [
        "# Mounting the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oa7Fm_pIRjd5"
      },
      "outputs": [],
      "source": [
        "# Loading the dataset with the proper delimiter (semicolon)\n",
        "data = pd.read_excel('/content/drive/MyDrive/Comprehensive_Chemical_Risk_Prediction_Model/IMPORT STATISTICS - 2023.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sLb40_ERmHc"
      },
      "outputs": [],
      "source": [
        "# Displaying the first few rows of the data\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0JSg4yoRrHl"
      },
      "outputs": [],
      "source": [
        "# Displaying information\n",
        "print(\"Displaying data information\")\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKJcjkaJRuce"
      },
      "outputs": [],
      "source": [
        "# Identifying categorical and numerical columns\n",
        "categorical_cols = data.select_dtypes(include=['object']).columns\n",
        "numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDQiGa86Rzue"
      },
      "outputs": [],
      "source": [
        "# Displaying the Categorical and the Numerical columns'\n",
        "print(\"Categorical Columns:\")\n",
        "print(categorical_cols)\n",
        "\n",
        "print(\"\\nNumerical Columns:\")\n",
        "print(numerical_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5WdVMqAR2h9"
      },
      "outputs": [],
      "source": [
        "# Converting 'IMPORTER' and 'HSCODE' columns to string type\n",
        "data['HSCODE'] = data['HSCODE'].astype(str)\n",
        "\n",
        "# Displaying the HSCODE column data type\n",
        "print(\"Data Type of HSCODE Columns:\")\n",
        "print(data[['HSCODE']].dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wytyIHToR59b"
      },
      "outputs": [],
      "source": [
        "# List of categorical features\n",
        "categorical_features = ['IMPORTER', 'MONTH', 'COUNTRY', 'HSCODE', 'UNIT', 'DESCRIPTION_01', 'DESCRIPTION_02', 'DESCRIPTION_03']\n",
        "\n",
        "# Displaying the categorical features\n",
        "print(\"Categorical Features:\")\n",
        "for feature in categorical_features:\n",
        "    print(f\"- {feature}\")\n",
        "\n",
        "# Display data type of the columns\n",
        "print(\"\\nData Type of Categorical Features:\")\n",
        "print(data[categorical_features].dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5cnbjbNR8Ym"
      },
      "outputs": [],
      "source": [
        "# Displaying the unique values and their counts relevant to each categorical column\n",
        "print(\"Unique values and their count relevant to each categorical column:\\n\")\n",
        "for col in categorical_features:\n",
        "    unique_values = data[col].unique()\n",
        "    value_counts = data[col].value_counts()\n",
        "    print(value_counts)\n",
        "    print(\" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIrR9X6OR_OD"
      },
      "outputs": [],
      "source": [
        "# Displaying the categorical columns which contains null values and their counts\n",
        "print(\"Categorical columns with null values and their counts:\")\n",
        "for col in categorical_features:\n",
        "    null_count = data[col].isnull().sum()\n",
        "    if null_count > 0:\n",
        "        print(f\"{col}: {null_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooeL5t2MSBgx"
      },
      "outputs": [],
      "source": [
        "# Displaying the categorical columns which contain 'Unknown' values and their relevant counts\n",
        "print(\"Categorical columns with 'Unknown' values and their counts:\")\n",
        "for col in categorical_features:\n",
        "    unknown_count = (data[col] == 'Unknown').sum()\n",
        "    if unknown_count > 0:\n",
        "        print(f\"{col}: {unknown_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ce0s8spZSEr0"
      },
      "outputs": [],
      "source": [
        "# Converting the 'Year' column to int64 data type\n",
        "# data['YEAR'] = data['YEAR'].astype(int)\n",
        "\n",
        "# Displaying the data type of the 'Year' column\n",
        "# print(\"Data Type of 'Year' Column:\")\n",
        "# print(data['YEAR'].dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTCh_JNLSHqw"
      },
      "outputs": [],
      "source": [
        "# List of numerical features\n",
        "numerical_features = ['YEAR', 'QUANTITY', 'VALUE_RS']\n",
        "\n",
        "# Displaying the numerical features\n",
        "print(\"Numerical Features:\")\n",
        "for feature in numerical_features:\n",
        "    print(f\"- {feature}\")\n",
        "\n",
        "# Display the data type of the numerical features\n",
        "print(\"\\nData Type of Numerical Features:\")\n",
        "print(data[numerical_features].dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CT0tHUB-SKL3"
      },
      "outputs": [],
      "source": [
        "# Displaying the unique values and their count in the numerical columns\n",
        "print(\"Unique values and their count in the numerical columns:\\n\")\n",
        "for col in numerical_features:\n",
        "    unique_values = data[col].unique()\n",
        "    value_counts = data[col].value_counts()\n",
        "    print(value_counts)\n",
        "    print(\" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mL2TVR5VSMv5"
      },
      "outputs": [],
      "source": [
        "# Displaying the numerical columns with null values and their relevant counts\n",
        "print(\"Numerical columns with null values and their relevant counts:\")\n",
        "for col in numerical_cols:\n",
        "    null_count = data[col].isnull().sum()\n",
        "    if null_count > 0:\n",
        "        print(f\"{col}: {null_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEra36rPSPBF"
      },
      "outputs": [],
      "source": [
        "# Step 1: Removing whitespaces from the object type columns\n",
        "object_columns = data.select_dtypes(include=['object']).columns\n",
        "data[object_columns] = data[object_columns].apply(lambda x: x.str.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X08Ns0uwSRFn"
      },
      "outputs": [],
      "source": [
        "# Step 2: Removing unnecessary full stops(\".\") from the categorical columns\n",
        "data[categorical_cols] = data[categorical_cols].apply(lambda x: x.str.replace('.', ''))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCTf04M2r0r5"
      },
      "outputs": [],
      "source": [
        "# Step 3: Removing unnecessary special characters and trailing spaces from the 'DESCRIPTION_03' column\n",
        "data['DESCRIPTION_03'] = data['DESCRIPTION_03'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
        "data['DESCRIPTION_03'] = data['DESCRIPTION_03'].str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ILWIdOS0OfY"
      },
      "outputs": [],
      "source": [
        "# Step 4: Removing trailing spaces for the entire 'DESCRIPTION_02' column\n",
        "data['DESCRIPTION_02'] = data['DESCRIPTION_02'].str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERYwXLY0STrB"
      },
      "outputs": [],
      "source": [
        "# Displaying the unique values and their counts relevant to each categorical column\n",
        "print(\"Unique values and their count relevant to each categorical column:\\n\")\n",
        "for col in categorical_features:\n",
        "    unique_values = data[col].unique()\n",
        "    value_counts = data[col].value_counts()\n",
        "    print(value_counts)\n",
        "    print(\" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WRCVTgJSXZ1"
      },
      "outputs": [],
      "source": [
        "# Step 5: Converting all the values in the 'COUNTRY' and the 'UNIT' columns to uppercase values\n",
        "data['COUNTRY'] = data['COUNTRY'].str.upper()\n",
        "data['UNIT'] = data['UNIT'].str.upper()\n",
        "\n",
        "# Displaying the modified dataset\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJk0FvSvSaLk"
      },
      "outputs": [],
      "source": [
        "# Step 6: Converting the 'KGS' values in the 'Unit' columns to 'KG'\n",
        "data.loc[data['UNIT'] == 'KGS', 'UNIT'] = 'KG'\n",
        "\n",
        "# Displaying the modified dataset\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6dhxlqYSdO_"
      },
      "outputs": [],
      "source": [
        "# Step 7: Handling the HSCODES\n",
        "\n",
        "# Checking the length of the HSCODES and determining the maximum length\n",
        "data['HSCODE_LENGTH'] = data['HSCODE'].apply(len)\n",
        "max_length = data['HSCODE_LENGTH'].max()\n",
        "\n",
        "# Padding HSCODE values with trailing zeros to match the maximum length\n",
        "data['HSCODE'] = data['HSCODE'].apply(lambda x: x.ljust(max_length, '0'))\n",
        "\n",
        "# Dropping the helper column 'HSCODE_LENGTH' as it's no longer needed\n",
        "data.drop(columns=['HSCODE_LENGTH'], inplace=True)\n",
        "\n",
        "# Displaying the modified dataset\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZ5JxIXg6fcG"
      },
      "outputs": [],
      "source": [
        "# Step 8: Handling the 'COUNTRY' Column\n",
        "\n",
        "# Replacing specific country names\n",
        "data['COUNTRY'] = data['COUNTRY'].replace({\n",
        "    'United States': 'USA',\n",
        "    'United States of America': 'USA',\n",
        "    'United Kingdom': 'UK',\n",
        "    'Korea, Republic of': 'South Korea'\n",
        "})\n",
        "\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxPMdYRKSivJ"
      },
      "outputs": [],
      "source": [
        "# Displaying the unique values and their counts relevant to each categorical column\n",
        "print(\"Unique values and their count relevant to each categorical column:\\n\")\n",
        "for col in categorical_features:\n",
        "    unique_values = data[col].unique()\n",
        "    value_counts = data[col].value_counts()\n",
        "    print(value_counts)\n",
        "    print(\" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqu-lfU2Sl-L"
      },
      "outputs": [],
      "source": [
        "# Step 9: Handling missing values in the categorical columns\n",
        "\n",
        "# Replacing all the missing values in the categorical columns with 'Unknown' for imputation\n",
        "data[categorical_features] = data[categorical_features].fillna('Unknown')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjyZ8UrHSnTj"
      },
      "outputs": [],
      "source": [
        "# Verifying the changes after replacing the missing values with 'Unknown'\n",
        "print(\"Checking for any missing values left behind after replacing with 'Unknown':\")\n",
        "for col in categorical_features:\n",
        "    null_count = data[col].isnull().sum()\n",
        "    if null_count > 0:\n",
        "        print(f\"{col}: {null_count} missing values\")\n",
        "    else:\n",
        "        print(f\"{col}: No missing values\")\n",
        "    print(\" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr14utC4H7BD"
      },
      "outputs": [],
      "source": [
        "# Handling the 'UNknown' values in the 'COUNTRY' column using the mode\n",
        "mode_country = data['COUNTRY'].mode()[0]\n",
        "data['COUNTRY'] = data['COUNTRY'].replace('Unknown', mode_country)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubphmTAASp_c"
      },
      "outputs": [],
      "source": [
        "# Displaying dataset information post-imputation\n",
        "print(\"\\nDataset information after categorical imputation:\")\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Epz9uWmBoR7X"
      },
      "outputs": [],
      "source": [
        "# Displaying the unique values in the 'IMPORTER' column\n",
        "unique_importers = data['IMPORTER'].unique()\n",
        "print(\"Unique Importers:\")\n",
        "print(unique_importers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bh41X8vjoYl-"
      },
      "outputs": [],
      "source": [
        "# Display the unique values in the 'YEAR' column\n",
        "unique_years = data['MONTH'].unique()\n",
        "print(\"Unique MONTH:\")\n",
        "print(unique_years)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-4Ls8YnopoZ"
      },
      "outputs": [],
      "source": [
        "# Display unique values in the 'COUNTRY' column\n",
        "unique_countries = data['COUNTRY'].unique()\n",
        "print(\"Unique Countries:\")\n",
        "print(unique_countries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koOoypOBowDQ"
      },
      "outputs": [],
      "source": [
        "# Display unique values in the 'HSCODE' column\n",
        "unique_hscodes = data['HSCODE'].unique()\n",
        "print(\"Unique HSCodes:\")\n",
        "print(unique_hscodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj5O3B2qofyU"
      },
      "outputs": [],
      "source": [
        "# Display the unique values in the 'UNIT' column\n",
        "unique_units = data['UNIT'].unique()\n",
        "print(\"Unique Units:\")\n",
        "print(unique_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhumfU-YpMR7"
      },
      "outputs": [],
      "source": [
        "# Displaying the unique values in the 'DESCRIPTION_01' column\n",
        "unique_descriptions_01 = data['DESCRIPTION_01'].unique()\n",
        "print(\"Unique Descriptions_01:\")\n",
        "print(unique_descriptions_01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OyLqe_NqPb1"
      },
      "outputs": [],
      "source": [
        "# Displaying the unique values in the 'DESCRIPTION_02' column\n",
        "unique_descriptions_02 = data['DESCRIPTION_02'].unique()\n",
        "print(\"Unique Descriptions_02:\")\n",
        "print(unique_descriptions_02)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTOexEiIqUzW"
      },
      "outputs": [],
      "source": [
        "# Displaying the unique values in the 'DESCRIPTION_03' column\n",
        "unique_descriptions_03 = data['DESCRIPTION_03'].unique()\n",
        "print(\"Unique Descriptions_03:\")\n",
        "print(unique_descriptions_03)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1bNJsIuStN_"
      },
      "outputs": [],
      "source": [
        "# Step 10: Encoding the Categorical Columns\n",
        "\n",
        "# List of categorical columns to encode (excluding 'DESCRIPTION_03' which will be handled separately)\n",
        "encoding_columns = ['IMPORTER', 'MONTH', 'COUNTRY', 'HSCODE', 'UNIT', 'DESCRIPTION_01', 'DESCRIPTION_02']\n",
        "\n",
        "# Creating a label encoder object\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Encoding and saving columns\n",
        "for col in encoding_columns:\n",
        "    # Encoding the column\n",
        "    data[col] = label_encoder.fit_transform(data[col])\n",
        "\n",
        "    # If the column is not one of the specific columns, save the encoder\n",
        "    if col not in ['IMPORTER', 'MONTH']:\n",
        "        with open(f'encoded_{col}.pkl', 'wb') as f:\n",
        "            pickle.dump(label_encoder, f)\n",
        "        print(f\"Encoded column: {col} saved successfully\")\n",
        "    else:\n",
        "        print(f\"Encoded column: {col}, but encoder not saved.\")\n",
        "\n",
        "# Separate encoding for 'DESCRIPTION_03'\n",
        "description_03_encoder = LabelEncoder()\n",
        "data['ENCODED_DESCRIPTION_03'] = description_03_encoder.fit_transform(data['DESCRIPTION_03'])\n",
        "\n",
        "# Saving the encoder for 'DESCRIPTION_03'\n",
        "with open('encoded_DESCRIPTION_03.pkl', 'wb') as f:\n",
        "    pickle.dump(description_03_encoder, f)\n",
        "\n",
        "print(\"Encoded column: DESCRIPTION_03 saved successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3oqY5pj7Piy"
      },
      "outputs": [],
      "source": [
        "# Displaying the categorical columns after the encoding\n",
        "print(\"Categorical columns after encoding:\")\n",
        "print(data[encoding_columns])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-yEqVFjSwcq"
      },
      "outputs": [],
      "source": [
        "# Imputing the 'MONTH' column with K-NN Imputer\n",
        "\n",
        "# Initialize the KNNImputer with 5 neighbors\n",
        "knn_imputer = KNNImputer(n_neighbors=5)\n",
        "\n",
        "# Applying the imputation to the 'MONTH' column\n",
        "data['MONTH'] = knn_imputer.fit_transform(data[['MONTH']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SD5_nJb_S207"
      },
      "outputs": [],
      "source": [
        "# Step 11: Handling missing values in the numerical columns\n",
        "\n",
        "# KNN Imputation for 'QUANTITY'\n",
        "knn_imputer = KNNImputer(n_neighbors=5)\n",
        "data[['QUANTITY']] = knn_imputer.fit_transform(data[['QUANTITY']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZ2ZD-DkS6XC"
      },
      "outputs": [],
      "source": [
        "# Preparing the data, using the separately encoded 'ENCODED_DESCRIPTION_03' and ignoring 'DESCRIPTION_03'\n",
        "missing_value_rs = data[data['VALUE_RS'].isnull()]\n",
        "non_missing_value_rs = data[data['VALUE_RS'].notnull()]\n",
        "\n",
        "# Dropping the 'VALUE_RS' and 'DESCRIPTION_03', use 'ENCODED_DESCRIPTION_03' instead\n",
        "X_value_rs = non_missing_value_rs.drop(columns=['VALUE_RS', 'DESCRIPTION_03'])\n",
        "X_value_rs['ENCODED_DESCRIPTION_03'] = non_missing_value_rs['ENCODED_DESCRIPTION_03']\n",
        "\n",
        "# Considering the 'VALUE_RS' as the target variable\n",
        "y_value_rs = non_missing_value_rs['VALUE_RS']\n",
        "\n",
        "# Training Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(random_state=42, n_estimators=100)\n",
        "rf_regressor.fit(X_value_rs, y_value_rs)\n",
        "\n",
        "# Preparing the missing data using 'ENCODED_DESCRIPTION_03' for prediction\n",
        "X_missing_value_rs = missing_value_rs.drop(columns=['VALUE_RS', 'DESCRIPTION_03'])\n",
        "X_missing_value_rs['ENCODED_DESCRIPTION_03'] = missing_value_rs['ENCODED_DESCRIPTION_03']\n",
        "\n",
        "# Predicting missing 'VALUE_RS' values and impute them back into the original dataframe\n",
        "data.loc[data['VALUE_RS'].isnull(), 'VALUE_RS'] = rf_regressor.predict(X_missing_value_rs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpARMjn_S92Q"
      },
      "outputs": [],
      "source": [
        "# Iterative Imputation for 'YEAR'\n",
        "iterative_imputer = IterativeImputer(random_state=42)\n",
        "data['YEAR'] = iterative_imputer.fit_transform(data[['YEAR', 'MONTH', 'QUANTITY', 'VALUE_RS']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuX-1fWdTAbL"
      },
      "outputs": [],
      "source": [
        "# Step 12: Handling the missing values in the 'IMPORTER' Column (Categorical Column)\n",
        "\n",
        "# Imputing the 'IMPORTER' column using the K-Means Clustering\n",
        "\n",
        "# Preparing features for clustering\n",
        "X = data[['COUNTRY', 'HSCODE', 'QUANTITY', 'VALUE_RS']]\n",
        "\n",
        "# Applying K-Means clustering\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)  # Adjust n_clusters as needed\n",
        "data['CLUSTER'] = kmeans.fit_predict(X)\n",
        "\n",
        "# Assigning importer names based on clusters\n",
        "importer_names = [\n",
        "    \"Global Traders Inc.\", \"Eastern Imports Ltd.\", \"Pacific Exports Co.\",\n",
        "    \"Summit Commerce\", \"EverGreen Enterprises\"\n",
        "]\n",
        "data['IMPORTER'] = data['CLUSTER'].apply(lambda x: importer_names[x])\n",
        "\n",
        "# Dropping the helper columns(encoded categorical columns)\n",
        "data = data.drop(columns=['COUNTRY', 'HSCODE', 'CLUSTER'])\n",
        "\n",
        "# Displaying the updated dataset\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zma89b2TDr0"
      },
      "outputs": [],
      "source": [
        "# Step 13: Handling the 'YEAR' Column\n",
        "# Converting the year data type into int64\n",
        "data['YEAR'] = data['YEAR'].astype(int)\n",
        "\n",
        "# Displaying the dataset information\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0LRj2OfTFrc"
      },
      "outputs": [],
      "source": [
        "# Step 14: Re-encode the Importer column\n",
        "label_encoder = LabelEncoder()\n",
        "data['IMPORTER'] = label_encoder.fit_transform(data['IMPORTER'])\n",
        "\n",
        "# Saving the encoder for 'IMPORTER' column\n",
        "with open('encoded_IMPORTER.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "# Displaying the encoded details\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upSYv0QpTJFZ"
      },
      "outputs": [],
      "source": [
        "# Step 15: Handling the outliers in the numerical columns\n",
        "\n",
        "# Creating an empty dictionary to store the outliers\n",
        "outlier_info = {}\n",
        "\n",
        "for column in numerical_features:\n",
        "    Q1, Q3 = data[column].quantile([0.25, 0.75])\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound, upper_bound = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
        "\n",
        "    # Detecting outliers\n",
        "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
        "\n",
        "    outlier_info[column] = {\n",
        "        'Outliers': outliers.shape[0],\n",
        "        'Lower Bound': lower_bound,\n",
        "        'Upper Bound': upper_bound\n",
        "    }\n",
        "\n",
        "    print(f\"Column: {column}\")\n",
        "    print(f\"  Outliers: {outlier_info[column]['Outliers']}, \"\n",
        "          f\"Lower: {lower_bound:.2f}, Upper: {upper_bound:.2f}\")\n",
        "    print(\" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziMsMAUMTMtB"
      },
      "outputs": [],
      "source": [
        "# Visualizing the outliers\n",
        "# Calculating the number of rows and columns based on the number of numerical columns\n",
        "num_columns = len(numerical_features)\n",
        "num_rows = int(np.ceil(num_columns / 3))\n",
        "\n",
        "# Creating a figure and axes array for subplots\n",
        "fig, axes = plt.subplots(num_rows, 3, figsize=(15, 5 * num_rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Plotting boxplots for each numerical column\n",
        "for i, column in enumerate(numerical_features):\n",
        "    ax = axes[i]\n",
        "\n",
        "    # Calculating IQR and bounds\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Plotting the boxplot\n",
        "    sns.boxplot(x=data[column], color='skyblue', flierprops=dict(marker='o', color='red', markersize=5), ax=ax)\n",
        "    ax.axvline(lower_bound, color='red', linestyle='--', label='Lower Bound')\n",
        "    ax.axvline(upper_bound, color='green', linestyle='--', label='Upper Bound')\n",
        "    ax.set_title(f'Boxplot of {column}')\n",
        "    ax.set_xlabel(column)\n",
        "    ax.legend()\n",
        "    ax.grid(axis='x', alpha=0.75)\n",
        "\n",
        "\n",
        "for i in range(num_columns, len(axes)):\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwZOvLZ8TP9r"
      },
      "outputs": [],
      "source": [
        "# Function to handle outliers by capping\n",
        "def handle_outliers(data, method='cap'):\n",
        "    for column in data.columns:\n",
        "        if column == 'YEAR':\n",
        "            lower_bound, upper_bound = -0.86, 0.50\n",
        "            data[column] = data[column].apply(lambda x: min(max(x, lower_bound), upper_bound) if method == 'cap' else x)\n",
        "\n",
        "        elif column == 'QUANTITY':\n",
        "            lower_bound, upper_bound = -0.31, 0.03\n",
        "            data[column] = data[column].apply(lambda x: min(max(x, lower_bound), upper_bound) if method == 'cap' else x)\n",
        "\n",
        "        elif column == 'VALUE_RS':\n",
        "            lower_bound, upper_bound = -0.39, -0.19\n",
        "            data[column] = data[column].apply(lambda x: min(max(x, lower_bound), upper_bound) if method == 'cap' else x)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "# Applying the function to handle outliers\n",
        "data_cleaned = handle_outliers(data.copy(), method='cap')\n",
        "\n",
        "# Displaying the cleaned dataset\n",
        "print(data_cleaned.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kUHUKoYTTBB"
      },
      "outputs": [],
      "source": [
        "# Step 16: Handling the multicollinearity in the numerical columns\n",
        "\n",
        "# Computing the correlation for numerical features\n",
        "correlation_matrix = data[numerical_features].corr()\n",
        "\n",
        "# Displaying the correlation matrix\n",
        "print(correlation_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If3OFYocWC0l"
      },
      "outputs": [],
      "source": [
        "# Plotting the correlation heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GQyW05GWFnc"
      },
      "outputs": [],
      "source": [
        "# Step 17: Assignining the Target Variable\n",
        "\n",
        "# Risk mapping for DESCRIPTION_03\n",
        "risk_mapping = {\n",
        "    'chlorine': 'Low',\n",
        "    'nitric acid; sulphonitric acids': 'Medium',\n",
        "    'cyanides and cyanide oxides: of sodium': 'High',\n",
        "    'cyanides and cyanide oxides: other': 'High',\n",
        "    'hydrogen peroxide, whether or not solidified with urea.': 'Medium',\n",
        "    'iodine': 'Low',\n",
        "    'bromine': 'Low',\n",
        "    'chlorates and perchlorates; bromates and perbromates; iodates and periodates': 'Medium',\n",
        "    'mineral or chemical fertilizers, nitrogenous': 'Low',\n",
        "    'sulphuric acid; oleum': 'Medium',\n",
        "    'halides and halide oxides of non metals': 'Low',\n",
        "    'fluorides; fluorosilicates, fluoroaluminates, and other complex fluorine salts': 'Low'\n",
        "}\n",
        "\n",
        "# Define quantity thresholds\n",
        "high_quantity_threshold = 10000\n",
        "medium_quantity_threshold = 5000\n",
        "\n",
        "# Function to calculate risk\n",
        "def calculate_risk(description, quantity):\n",
        "    description_risk = risk_mapping.get(description.lower(), 'Low')\n",
        "    if description_risk == 'High' or quantity > high_quantity_threshold:\n",
        "        return 'High'\n",
        "    elif description_risk == 'Medium' or quantity > medium_quantity_threshold:\n",
        "        return 'Medium'\n",
        "    else:\n",
        "        return 'Low'\n",
        "\n",
        "# Applying the function\n",
        "data['RISK'] = data.apply(\n",
        "    lambda row: calculate_risk(row['DESCRIPTION_03'], row['QUANTITY']),\n",
        "    axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHoph2DiWIhq"
      },
      "outputs": [],
      "source": [
        "# Visualizing the target variable\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='RISK', data=data)\n",
        "plt.title('Risk Distribution')\n",
        "plt.xlabel('Risk')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ORBRZr2WLSV"
      },
      "outputs": [],
      "source": [
        "# Step 18: Encoding the 'DESCRIPTION_03' column with label encoder and dropping the 'ENCODED_DESCRIPTION_03' column\n",
        "label_encoder = LabelEncoder()\n",
        "data['DESCRIPTION_03'] = label_encoder.fit_transform(data['DESCRIPTION_03'])\n",
        "data.drop(columns=['ENCODED_DESCRIPTION_03'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkM9P9hYWMo2"
      },
      "outputs": [],
      "source": [
        "# Step 19: Encoding the 'RISK' variable\n",
        "label_encoder = LabelEncoder()\n",
        "data['RISK'] = label_encoder.fit_transform(data['RISK'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMGuuEaoWN-q"
      },
      "outputs": [],
      "source": [
        "# Step 20: Scaling the numerical features\n",
        "scaler = StandardScaler()\n",
        "data[numerical_features] = scaler.fit_transform(data[numerical_features])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpMu6UGGWPMS"
      },
      "outputs": [],
      "source": [
        "# Step 21: Splitting features and target\n",
        "X = data.drop(columns=['RISK'])\n",
        "y = data['RISK']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oy97kZpfWTIT"
      },
      "outputs": [],
      "source": [
        "# Step 22: Splitting data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0Sv-J7iWWAK"
      },
      "outputs": [],
      "source": [
        "# Analyzing the distribution of the y train dataset\n",
        "unique_values, counts = np.unique(y_train, return_counts=True)\n",
        "for value, count in zip(unique_values, counts):\n",
        "    print(f\"Value: {value}, Count: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIk63PNEWYaW"
      },
      "outputs": [],
      "source": [
        "# Visualizing the distribution of the target variable\n",
        "sns.countplot(data=data, x='RISK')\n",
        "plt.title('Target Variable Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdHFke_fWbY_"
      },
      "outputs": [],
      "source": [
        "# Displaying the dataset shape\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsvewUCZWclb"
      },
      "outputs": [],
      "source": [
        "# Display dataset information\n",
        "print(\"Dataset Information:\")\n",
        "print(data.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnGcn6YiWfKN"
      },
      "outputs": [],
      "source": [
        "# Display dataset information\n",
        "print(\"Dataset Information:\")\n",
        "print(data.head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJALpjd5WiXf"
      },
      "outputs": [],
      "source": [
        "# Handling the imbalance dataset\n",
        "\n",
        "# Suppressing the FutureWarnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Handling imbalanced dataset\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "print(\"Original dataset size:\", X.shape)\n",
        "print(\"Resampled dataset size:\", X_resampled.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUwdji5AWlGO"
      },
      "outputs": [],
      "source": [
        "# Plotting class distribution after SMOTE\n",
        "sns.countplot(x=y_resampled, palette=\"viridis\")\n",
        "plt.title(\"Class Distribution After SMOTE\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DV3t_x-Wnap"
      },
      "outputs": [],
      "source": [
        "# Displaying the dataset shape after applying the SMOTE\n",
        "print(f\"Shape of X_resampled: {X_resampled.shape}\")\n",
        "print(f\"Shape of y_resampled: {y_resampled.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fq-6bCFMWoxd"
      },
      "outputs": [],
      "source": [
        "# Standardizing the features\n",
        "scaler = StandardScaler()\n",
        "X_resampled_scaled = scaler.fit_transform(X_resampled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKKCxYzoWqBW"
      },
      "outputs": [],
      "source": [
        "# Splitting data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled_scaled, y_resampled, test_size=0.2, stratify = y_resampled, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TT2I8hvWr3K"
      },
      "outputs": [],
      "source": [
        "# Display dataset information\n",
        "print(\"Dataset Information:\")\n",
        "print(data.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTkSM9zLWuKR"
      },
      "outputs": [],
      "source": [
        "# Scaling the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skZKRUh-WvU7"
      },
      "outputs": [],
      "source": [
        "# Splitting the data into training (60%) and temporary (40%) sets\n",
        "X_train_scaled, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.4, random_state=42, stratify=y)\n",
        "\n",
        "# Splitting the temporary set into validation (50% of temp, i.e., 20% of total) and test (50% of temp, i.e., 20% of total)\n",
        "X_val_scaled, X_test_scaled, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLfYLaWhWxNT"
      },
      "outputs": [],
      "source": [
        "# Standard scaling for the splits\n",
        "X_train_scaled = scaler.fit_transform(X_train_scaled)\n",
        "X_val_scaled = scaler.transform(X_val_scaled)\n",
        "X_test_scaled = scaler.transform(X_test_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZEs_ApuW1Ra"
      },
      "outputs": [],
      "source": [
        "# Setting a global seed for reproducibility\n",
        "def set_seed(seed_value=42):\n",
        "    np.random.seed(seed_value)\n",
        "    random.seed(seed_value)\n",
        "    tf.random.set_seed(seed_value)\n",
        "\n",
        "# Defining the LSTM model\n",
        "def create_lstm_model(input_shape, seed_value=42):\n",
        "    # Setting the seed before building the model\n",
        "    set_seed(seed_value)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=64, return_sequences=False, input_shape=input_shape))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ya1G8atHW2es"
      },
      "outputs": [],
      "source": [
        "# Reshaping input data for LSTM (3D input required: samples, timesteps, features)\n",
        "X_train_lstm = X_train_scaled.reshape(X_train_scaled.shape[0], 1, X_train_scaled.shape[1])\n",
        "X_val_lstm = X_val_scaled.reshape(X_val_scaled.shape[0], 1, X_val_scaled.shape[1])\n",
        "X_test_lstm = X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])\n",
        "\n",
        "# Creating and train the LSTM model\n",
        "lstm_model = create_lstm_model((X_train_lstm.shape[1], X_train_lstm.shape[2]))\n",
        "lstm_model.fit(X_train_lstm, y_train, epochs=50, batch_size=32, validation_data=(X_val_lstm, y_val))\n",
        "\n",
        "# Extracting LSTM features (use LSTM's output as features for Gradient Boosting)\n",
        "lstm_train_features = lstm_model.predict(X_train_lstm)\n",
        "lstm_val_features = lstm_model.predict(X_val_lstm)\n",
        "lstm_test_features = lstm_model.predict(X_test_lstm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izAjaecjW627"
      },
      "outputs": [],
      "source": [
        "# Reshape features for Gradient Boosting\n",
        "lstm_train_features = lstm_train_features.reshape(-1, 1)\n",
        "lstm_val_features = lstm_val_features.reshape(-1, 1)\n",
        "lstm_test_features = lstm_test_features.reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "6smI81k1W8Hu",
        "outputId": "e1cbeaf4-81fc-462e-d8c2-e39b5a5697d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
            "Best Parameters for Gradient Boosting:  {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 50}\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameter tuning for Gradient Boosting using GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Grid search cross-validation for Gradient Boosting\n",
        "grid_search = GridSearchCV(GradientBoostingClassifier(), param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "grid_search.fit(lstm_train_features, y_train)\n",
        "\n",
        "# Best parameters from grid search\n",
        "print(\"Best Parameters for Gradient Boosting: \", grid_search.best_params_)\n",
        "\n",
        "# Train Gradient Boosting on the best parameters\n",
        "gb_model_class = grid_search.best_estimator_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BqlVPG3wXAV-",
        "outputId": "7dcc99f2-eea4-41df-e93b-7632c52ee7dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validation Accuracy: 0.9696 ± 0.005986651818838311\n"
          ]
        }
      ],
      "source": [
        "# Cross-validation on Gradient Boosting\n",
        "cv_scores = cross_val_score(gb_model_class, lstm_train_features, y_train, cv=StratifiedKFold(5), scoring='accuracy')\n",
        "print(f\"Cross-validation Accuracy: {np.mean(cv_scores)} ± {np.std(cv_scores)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQ_9guDXXBpT"
      },
      "outputs": [],
      "source": [
        "# Predicting on test data\n",
        "y_pred_class = gb_model_class.predict(lstm_test_features)\n",
        "\n",
        "# Evaluation for classification\n",
        "print(\"\\nClassification Evaluation:\")\n",
        "print(\"Accuracy: \", accuracy_score(y_test, y_pred_class))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_class))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiHMHAfVXEqD"
      },
      "outputs": [],
      "source": [
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_class)\n",
        "\n",
        "# Plotting the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=['Class 0', 'Class 1', 'Class 2'], yticklabels=['Class 0', 'Class 1', 'Class 2'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-ggWYefXHUO"
      },
      "outputs": [],
      "source": [
        "# Overfitting Score: Train vs Test Accuracy\n",
        "y_pred_train = gb_model_class.predict(lstm_train_features)\n",
        "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
        "print(\"Train Accuracy: \", train_accuracy)\n",
        "\n",
        "y_pred_test = gb_model_class.predict(lstm_test_features)\n",
        "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "print(\"Test Accuracy: \", test_accuracy)\n",
        "\n",
        "# Check for overfitting\n",
        "if train_accuracy - test_accuracy > 0.1:\n",
        "    print(\"The model might be overfitting. Consider regularization, reducing model complexity, or adding more data.\")\n",
        "else:\n",
        "    print(\"The model has balanced performance on train and test sets.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWLyfipQXJZZ"
      },
      "outputs": [],
      "source": [
        "# Saving Scaler\n",
        "joblib.dump(scaler, '/content/drive/MyDrive/Comprehensive_Chemical_Risk_Prediction_Model/scaler.pkl')\n",
        "\n",
        "# Saving LSTM Model\n",
        "lstm_model.save('/content/drive/MyDrive/Comprehensive_Chemical_Risk_Prediction_Model/lstm_model.h5')\n",
        "\n",
        "# Saving Gradient Boosting Model\n",
        "joblib.dump(gb_model_class, '/content/drive/MyDrive/Comprehensive_Chemical_Risk_Prediction_Model/gb_model_class.pkl')\n",
        "\n",
        "print(\"Models saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uj6pF0wcXLaV"
      },
      "outputs": [],
      "source": [
        "# Loading the Scaler\n",
        "scaler = joblib.load('/content/drive/MyDrive/Comprehensive_Chemical_Risk_Prediction_Model/scaler.pkl')\n",
        "\n",
        "# Loading the LSTM Model\n",
        "lstm_model = load_model('/content/drive/MyDrive/Comprehensive_Chemical_Risk_Prediction_Model/lstm_model.h5')\n",
        "\n",
        "# Loading the Gradient Boosting Model\n",
        "gb_model_class = joblib.load('/content/drive/MyDrive/Comprehensive_Chemical_Risk_Prediction_Model/gb_model_class.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {
        "id": "fCzmJRGjXOgO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "7bdfd0bb-f768-4580-f8f0-5de692140234"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'scaler.pkl'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-252-4d230b7248fe>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Load pre-trained models and scalers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'scaler.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mlstm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lstm_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mgb_model_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gb_model.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'scaler.pkl'"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import numpy as np\n",
        "import joblib\n",
        "from keras.models import load_model\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Define encoding lists\n",
        "importer_list = [\"Global Traders Inc.\", \"Eastern Imports Ltd.\", \"Pacific Exports Co.\",\n",
        "                 \"Summit Commerce\", \"EverGreen Enterprises\"]\n",
        "month_list = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
        "country_list = [\"USA\", \"INDIA\", \"CHINA\", \"VIETNAM\", \"RUSSIA\", \"KENYA\", \"BELGIUM\", \"SOUTH KOREA\",\n",
        "                \"UKRAINE\", \"GERMANY\", \"COLOMBIA\", \"BANGLADESH\", \"SLOVENIA\", \"MALAYSIA\", \"US VIRGIN ISLANDS\",\n",
        "                \"URUGUAY\", \"THAILAND\", \"JAPAN\", \"SRI LANKA\", \"GERMENY\", \"CAMBODIA\", \"ISRAEL\", \"SWITZERLAND\",\n",
        "                \"UK\", \"CANADA\", \"TAIWAN\", \"TURKMENISTAN\", \"CHILE\", \"SINGAPORE\", \"FRANCE\", \"INDONESIA\",\n",
        "                \"KAZAKHSTAN\", \"NIGERIA\", \"SOUTH AFRICA\", \"UAE\", \"SPAIN\", \"SAUDI ARABIA\", \"ARGENTINA\", \"BRAZIL\",\n",
        "                \"ITALY\", \"EGYPT\", \"SLOVAKIA\", \"ROMANIA\", \"AUSTRIA\", \"MEXICO\", \"DENMARK\", \"PAKISTAN\", \"NETHERLANDS (HOLAND)\", \"GEORGIA\"]\n",
        "unit_list = [\"KG\", \"PCS\", \"NOS\", \"GMS\", \"MTS\", \"LTR\", \"DRM\"]\n",
        "\n",
        "# Initialize encoders for categorical values\n",
        "importer_encoder = LabelEncoder().fit(importer_list)\n",
        "month_encoder = LabelEncoder().fit(month_list)\n",
        "country_encoder = LabelEncoder().fit(country_list)\n",
        "unit_encoder = LabelEncoder().fit(unit_list)\n",
        "\n",
        "# Load pre-trained models and scalers\n",
        "scaler = joblib.load('scaler.pkl')\n",
        "lstm_model = load_model('lstm_model.h5')\n",
        "gb_model_class = joblib.load('gb_model.pkl')\n",
        "\n",
        "# Risk mapping dictionary\n",
        "risk_mapping = {\n",
        "    'chlorine': 'Low',\n",
        "    'nitric acid; sulphonitric acids': 'Medium',\n",
        "    'cyanides and cyanide oxides: of sodium': 'High',\n",
        "    'hydrogen peroxide, whether or not solidified with urea.': 'Medium',\n",
        "    'iodine': 'Low',\n",
        "    'bromine': 'Low',\n",
        "    'chlorates and perchlorates; bromates and perbromates; iodates and periodates': 'Medium',\n",
        "    'mineral or chemical fertilizers, nitrogenous': 'Low',\n",
        "    'sulphuric acid; oleum': 'Medium',\n",
        "    'halides and halide oxides of non metals': 'Low',\n",
        "    'fluorides; fluorosilicates, fluoroaluminates, and other complex fluorine salts': 'Low'\n",
        "}\n",
        "\n",
        "# Define quantity thresholds\n",
        "high_quantity_threshold = 10000\n",
        "medium_quantity_threshold = 5000\n",
        "\n",
        "# Function to encode the inputs\n",
        "def encode_inputs(importer_input, month_input, country_input, unit_input, year_input, hscode_input, description_3_input, quantity_input, value_input):\n",
        "    try:\n",
        "        importer_encoded = importer_encoder.transform([importer_input])[0] if importer_input in importer_list else len(importer_list)\n",
        "        month_encoded = month_encoder.transform([month_input])[0] if month_input in month_list else len(month_list)\n",
        "        country_encoded = country_encoder.transform([country_input])[0] if country_input in country_list else len(country_list)\n",
        "        unit_encoded = unit_encoder.transform([unit_input])[0] if unit_input in unit_list else len(unit_list)\n",
        "\n",
        "        return (importer_encoded, month_encoded, country_encoded, unit_encoded, year_input, hscode_input, len(description_3_input), quantity_input, value_input)\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Encoding failed: {e}\")\n",
        "\n",
        "# Function to calculate risk\n",
        "def calculate_risk(description, quantity):\n",
        "    description_risk = risk_mapping.get(description.lower(), 'Low')\n",
        "    if description_risk == 'High' or quantity > high_quantity_threshold:\n",
        "        return 'High'\n",
        "    elif description_risk == 'Medium' or quantity > medium_quantity_threshold:\n",
        "        return 'Medium'\n",
        "    else:\n",
        "        return 'Low'\n",
        "\n",
        "# API endpoint for risk assessment\n",
        "@app.route('/check-risk', methods=['POST'])\n",
        "def check_risk():\n",
        "    try:\n",
        "        # Get JSON data from the request\n",
        "        data = request.get_json()\n",
        "\n",
        "        # Extract input data\n",
        "        importer_input = data.get('importer')\n",
        "        month_input = data.get('month')\n",
        "        country_input = data.get('country')\n",
        "        description_3_input = data.get('description')\n",
        "        quantity_input = data.get('quantity', 0)\n",
        "        value_input = data.get('value', 0)\n",
        "\n",
        "        # Validate inputs\n",
        "        if not all([importer_input, month_input, country_input, description_3_input]):\n",
        "            return jsonify({'error': 'Missing required inputs'}), 400\n",
        "\n",
        "        # Encode inputs\n",
        "        encoded_values = encode_inputs(importer_input, month_input, country_input, \"KG\", 2025, 28080000, description_3_input, quantity_input, value_input)\n",
        "\n",
        "        # Preprocess features for models\n",
        "        features = np.array(encoded_values).reshape(1, -1)\n",
        "        features_scaled = scaler.transform(features)\n",
        "\n",
        "        # Prepare LSTM input format\n",
        "        features_lstm = features_scaled.reshape(features_scaled.shape[0], 1, features_scaled.shape[1])\n",
        "\n",
        "        # LSTM feature extraction\n",
        "        lstm_features = lstm_model.predict(features_lstm)\n",
        "        lstm_features = lstm_features.reshape(-1, 1)\n",
        "\n",
        "        # Risk classification using Gradient Boosting\n",
        "        prediction = gb_model_class.predict(lstm_features)\n",
        "        classification = 'risky' if prediction == 1 else 'not risky'\n",
        "\n",
        "        # Risk category assessment\n",
        "        risk_category = calculate_risk(description_3_input, quantity_input)\n",
        "\n",
        "        # Return the response\n",
        "        return jsonify({\n",
        "            'risk': risk_category,\n",
        "            'classification': classification\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "# Run the app\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HT4QPq9DPIo"
      },
      "outputs": [],
      "source": [
        "# Step 7: Encoding the Categorical Columns\n",
        "\n",
        "# List of categorical columns to encode\n",
        "encoding_columns = ['IMPORTER', 'MONTH', 'COUNTRY', 'HSCODE', 'UNIT', 'DESCRIPTION_01', 'DESCRIPTION_02', 'DESCRIPTION_03']\n",
        "\n",
        "# Define the mapping dictionaries\n",
        "importer_map = {\n",
        "    \"Global Traders Inc.\": 0,\n",
        "    \"Eastern Imports Ltd.\": 1,\n",
        "    \"Pacific Exports Co.\": 2,\n",
        "    \"Summit Commerce\": 3,\n",
        "    \"EverGreen Enterprises\": 4\n",
        "}\n",
        "\n",
        "month_map = {\n",
        "    \"January\": 0,\n",
        "    \"February\": 1,\n",
        "    \"March\": 2,\n",
        "    \"April\": 3,\n",
        "    \"May\": 4,\n",
        "    \"June\": 5,\n",
        "    \"July\": 6,\n",
        "    \"August\": 7,\n",
        "    \"September\": 8,\n",
        "    \"October\": 9,\n",
        "    \"November\": 10,\n",
        "    \"December\": 11\n",
        "}\n",
        "\n",
        "country_map = {\n",
        "    \"USA\": 0, \"INDIA\": 1, \"CHINA\": 2, \"VIETNAM\": 3, \"RUSSIA\": 4, \"KENYA\": 5, \"BELGIUM\": 6,\n",
        "    \"SOUTH KOREA\": 7, \"UKRAINE\": 8, \"GERMANY\": 9, \"COLOMBIA\": 10, \"BANGLADESH\": 11,\n",
        "    \"SLOVENIA\": 12, \"MALAYSIA\": 13, \"US VIRGIN ISLANDS\": 14, \"URUGUAY\": 15, \"THAILAND\": 16,\n",
        "    \"JAPAN\": 17, \"SRI LANKA\": 18, \"GERMENY\": 19, \"CAMBODIA\": 20, \"ISRAEL\": 21, \"SWITZERLAND\": 22,\n",
        "    \"UK\": 23, \"CANADA\": 24, \"TAIWAN\": 25, \"TURKMENISTAN\": 26, \"CHILE\": 27, \"SINGAPORE\": 28,\n",
        "    \"FRANCE\": 29, \"INDONESIA\": 30, \"KAZAKHSTAN\": 31, \"NIGERIA\": 32, \"SOUTH AFRICA\": 33,\n",
        "    \"UAE\": 34, \"SPAIN\": 35, \"SAUDI ARABIA\": 36, \"ARGENTINA\": 37, \"BRAZIL\": 38, \"ITALY\": 39,\n",
        "    \"EGYPT\": 40, \"SLOVAKIA\": 41, \"ROMANIA\": 42, \"AUSTRIA\": 43, \"MEXICO\": 44, \"DENMARK\": 45,\n",
        "    \"PAKISTAN\": 46, \"NETHERLANDS (HOLAND)\": 47, \"GEORGIA\": 48\n",
        "}\n",
        "\n",
        "unit_map = {\n",
        "    \"KG\": 0,\n",
        "    \"PCS\": 1,\n",
        "    \"NOS\": 2,\n",
        "    \"GMS\": 3,\n",
        "    \"MTS\": 4,\n",
        "    \"LTR\": 5,\n",
        "    \"DRM\": 6\n",
        "}\n",
        "\n",
        "# DESCRIPTION_03 Encoding Map\n",
        "description_03_map = {\n",
        "    'chlorine': 0,\n",
        "    'nitric acid; sulphonitric acids': 1,\n",
        "    'cyanides and cyanide oxides: of sodium': 2,\n",
        "    'hydrogen peroxide, whether or not solidified with urea.': 3,\n",
        "    'iodine': 4,\n",
        "    'bromine': 5,\n",
        "    'chlorates and perchlorates; bromates and perbromates; iodates and periodates': 6,\n",
        "    'mineral or chemical fertilizers, nitrogenous': 7,\n",
        "    'sulphuric acid; oleum': 8,\n",
        "    'halides and halide oxides of non metals': 9,\n",
        "    'fluorides; fluorosilicates, fluoroaluminates, and other complex fluorine salts': 10,\n",
        "    'cyanides, cyanide oxides and complex cyanides': 11,\n",
        "}\n",
        "\n",
        "# Mappings for DESCRIPTION_01\n",
        "description_01_map = {\n",
        "    'Inorganic chemicals; Organic or inorganic compounds of precious metals, of rare earth metals, of radioactive elements or of isotopes': 0,\n",
        "    'Fertilizers': 1,\n",
        "}\n",
        "\n",
        "# Mappings for DESCRIPTION_02\n",
        "description_02_map = {\n",
        "    'Nitric acid; sulphonitric acids': 0,\n",
        "    'Chlorates and perchlorates; bromates and perbromates; iodates and periodates': 1,\n",
        "    'Mineral or chemical fertilizers, nitrogenous': 2,\n",
        "    'Sulphuric acid; oleum': 3,\n",
        "    'Halides and halide oxides of non metals': 4,\n",
        "    'Fluorides; fluorosilicates, fluoroaluminates, and other complex fluorine salts': 5,\n",
        "    'Hydrogen peroxide, whether or not solidified with urea': 6,\n",
        "    'Fluorine, chlorine, bromine and iodine': 7,\n",
        "    'Cyanides, cyanide oxides and complex cyanides': 8,\n",
        "}\n",
        "\n",
        "# Applying mappings manually for specific columns\n",
        "data['IMPORTER'] = data['IMPORTER'].map(importer_map)\n",
        "data['MONTH'] = data['MONTH'].map(month_map)\n",
        "data['COUNTRY'] = data['COUNTRY'].map(country_map)\n",
        "data['UNIT'] = data['UNIT'].map(unit_map)\n",
        "data['DESCRIPTION_01'] = data['DESCRIPTION_01'].map(description_01_map)\n",
        "data['DESCRIPTION_02'] = data['DESCRIPTION_02'].map(description_02_map)\n",
        "\n",
        "# Encoding `DESCRIPTION_03` separately using the pre-defined map\n",
        "data['ENCODED_DESCRIPTION_03'] = data['DESCRIPTION_03'].map(description_03_map)\n",
        "print(data[['DESCRIPTION_03', 'ENCODED_DESCRIPTION_03']])\n",
        "print(\" \")\n",
        "\n",
        "# Encoding 'HSCODE' column separately\n",
        "data['HSCODE'] = label_encoder.fit_transform(data['HSCODE'])\n",
        "print(\"Encoded column: HSCODE\")\n",
        "print(data['HSCODE'])\n",
        "print(\" \")\n",
        "\n",
        "# Display all the encoded columns\n",
        "print(\"All Encoded Columns:\")\n",
        "print(data)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZVs718wCfstXI2P2ty9gM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}