{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMp7e3ZVGmfszFHkaqoV6p3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShabnaIlmi/Data-Science-Group-Project/blob/Importer_Risk_Prediction_02/Exploratary_Data_Analysis_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Installing Necessary Libraries**"
      ],
      "metadata": {
        "id": "OYYTge9I8GPT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OPoSbhIC6UZQ"
      },
      "outputs": [],
      "source": [
        "# Installing Required Libraries\n",
        "# !pip install --upgrade tensorflow\n",
        "# !pip install fancyimpute scikit-learn pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing the Relevant Libraries**"
      ],
      "metadata": {
        "id": "h--5ArzY8O4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.cluster import KMeans\n",
        "import warnings\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import shap\n",
        "import pickle\n",
        "import joblib"
      ],
      "metadata": {
        "id": "WyX8k6ktR4jH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "91699a84-ab00-4182-f67c-f1fd8c5fc32f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "partially initialized module 'pandas' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-d91bcfadd74c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKNNImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/seaborn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import seaborn objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrcmod\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpalettes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrelational\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/seaborn/rcmod.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcycler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcycler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpalettes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/seaborn/palettes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhusl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdesaturate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_color_cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcolors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxkcd_rgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrayons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_compat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_colormap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/seaborn/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_rgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    136\u001b[0m )\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplotting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtseries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/api/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\" public toolkit API \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m from pandas.api import (\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mindexers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minterchange\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/api/typing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# TODO: Can't import Styler without importing jinja2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# from pandas.io.formats.style import Styler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mJsonReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStataReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/json/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from pandas.io.json._json import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mread_json\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mto_json\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mujson_dumps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mujson_loads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mparse_table_schema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m )\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from pandas.io.parsers.readers import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mTextFileReader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mTextParser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mread_csv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mread_fwf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_libs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_libs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSTR_NA_VALUES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m from pandas.errors import (\n\u001b[1;32m     34\u001b[0m     \u001b[0mAbstractMethodError\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36minit pandas._libs.parsers\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'pandas' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting the Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HpksTyec8ie0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading the Dataset**"
      ],
      "metadata": {
        "id": "mWzlfw6T8knI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the dataset with the proper delimiter (semicolon)\n",
        "data = pd.read_excel('/content/drive/MyDrive/Importer_Risk_Prediction_2/Dataset/IMPORT STATISTICS - 2023.xlsx')"
      ],
      "metadata": {
        "id": "qN8eyZZE8ooa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the first few rows of the data\n",
        "data.head()"
      ],
      "metadata": {
        "id": "KTvjjfnA8qjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploratory Data Analysis**"
      ],
      "metadata": {
        "id": "UO79Bapj9j8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying information\n",
        "print(\"Displaying data information\")\n",
        "data.info()"
      ],
      "metadata": {
        "id": "tqzoA-ED9mFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Identifying Categorical and Numerical Columns**"
      ],
      "metadata": {
        "id": "wlGynzWyAi3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identifying categorical and numerical columns\n",
        "categorical_cols = data.select_dtypes(include=['object']).columns\n",
        "numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns"
      ],
      "metadata": {
        "id": "YnzurPy5Ak3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Since 'HSCODE' column has been misintepreted as an int64 data type column due to the unavailability of data, reassigning it has an object type column.**"
      ],
      "metadata": {
        "id": "BMmSEcp9AmfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting 'IMPORTER' and 'HSCODE' columns to string type\n",
        "data['HSCODE'] = data['HSCODE'].astype(str)\n",
        "\n",
        "# Displaying the HSCODE column data type\n",
        "print(\"Data Type of HSCODE Columns:\")\n",
        "print(data[['HSCODE']].dtypes)"
      ],
      "metadata": {
        "id": "QWxIkJ7sAqCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Categorical Features**"
      ],
      "metadata": {
        "id": "MAlAa-aoAs_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of categorical features\n",
        "categorical_features = data.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Displaying the categorical features\n",
        "print(\"Categorical Features:\")\n",
        "for feature in categorical_features:\n",
        "    print(f\"- {feature}\")\n",
        "\n",
        "# Display data type of the columns\n",
        "print(\"\\nData Type of Categorical Features:\")\n",
        "print(data[categorical_features].dtypes)"
      ],
      "metadata": {
        "id": "f6CqNJqnAxxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unique Values and Their Counts Relevant to Each Categorical Column**"
      ],
      "metadata": {
        "id": "TWNh9w1BA1NB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the unique values and their counts relevant to each categorical column\n",
        "print(\"Unique values and their count relevant to each categorical column:\\n\")\n",
        "for col in categorical_features:\n",
        "    unique_values = data[col].unique()\n",
        "    value_counts = data[col].value_counts()\n",
        "    print(value_counts)\n",
        "    print(\" \")"
      ],
      "metadata": {
        "id": "P4FVhDRTA3oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the categorical columns which contains null values and their counts\n",
        "print(\"Categorical columns with null values and their counts:\")\n",
        "for col in categorical_features:\n",
        "    null_count = data[col].isnull().sum()\n",
        "    if null_count > 0:\n",
        "        print(f\"{col}: {null_count}\")"
      ],
      "metadata": {
        "id": "GioatFqCA7ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the categorical columns which contain 'Unknown' values and their relevant counts\n",
        "print(\"Categorical columns with 'Unknown' values and their counts:\")\n",
        "for col in categorical_features:\n",
        "    unknown_count = (data[col] == 'Unknown').sum()\n",
        "    if unknown_count > 0:\n",
        "        print(f\"{col}: {unknown_count}\")"
      ],
      "metadata": {
        "id": "xdC5PHXgA9Fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Numerical Features**"
      ],
      "metadata": {
        "id": "cS1RfvF5BBO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Since 'Year' column has been misintepreted as a float64 data type column reassigning it as a int64 type column**"
      ],
      "metadata": {
        "id": "5JiGaPjBBENa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the 'Year' column to int64 data type\n",
        "# data['YEAR'] = data['YEAR'].astype(int)\n",
        "\n",
        "# Displaying the data type of the 'Year' column\n",
        "# print(\"Data Type of 'Year' Column:\")\n",
        "# print(data['YEAR'].dtype)"
      ],
      "metadata": {
        "id": "xoZXpCjSBGV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The conversion cannot be done since the column contains missing values (both N/A and null values)**"
      ],
      "metadata": {
        "id": "k61Ebt72BIUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Numerical Features\n",
        "numerical_features = data.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Displaying the Numerical Columns\n",
        "print(\"Numerical Features:\")\n",
        "print(numerical_features)"
      ],
      "metadata": {
        "id": "yHDT8jBYBd4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unique Values and Their Count Relevant to Each Numerical Column**"
      ],
      "metadata": {
        "id": "ZpzIVQjABhC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the unique values and their count in the numerical columns\n",
        "print(\"Unique values and their count in the numerical columns:\\n\")\n",
        "for col in numerical_features:\n",
        "    unique_values = data[col].unique()\n",
        "    value_counts = data[col].value_counts()\n",
        "    print(value_counts)\n",
        "    print(\" \")"
      ],
      "metadata": {
        "id": "NrYaMzC7Bq01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Numerical columns with null values and their relevant counts**"
      ],
      "metadata": {
        "id": "YFjllzAsBuT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the numerical columns with null values and their relevant counts\n",
        "print(\"Numerical columns with null values and their relevant counts:\")\n",
        "for col in numerical_cols:\n",
        "    null_count = data[col].isnull().sum()\n",
        "    if null_count > 0:\n",
        "        print(f\"{col}: {null_count}\")"
      ],
      "metadata": {
        "id": "nd9l3Ut1BwXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**"
      ],
      "metadata": {
        "id": "79zB4WHtCOdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Cleaning**"
      ],
      "metadata": {
        "id": "uIFg1V0kCSXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Removing whitespaces from the object type columns\n",
        "object_columns = data.select_dtypes(include=['object']).columns\n",
        "data[object_columns] = data[object_columns].apply(lambda x: x.str.strip())"
      ],
      "metadata": {
        "id": "OfzJDJDGCWsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Dropping duplicate values\n",
        "data.drop_duplicates(inplace=True)\n",
        "data.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "wViKEDcAsajd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display dataset information\n",
        "print(\"\\nDataset information after removing duplicates:\")\n",
        "data.info()"
      ],
      "metadata": {
        "id": "gRf6uNsWBcW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Removing unnecessary full stops(\".\") from the categorical columns\n",
        "data[categorical_cols] = data[categorical_cols].apply(lambda x: x.str.replace('.', ''))"
      ],
      "metadata": {
        "id": "1u7mE0RGCYtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Removing unnecessary special characters and trailing spaces from the 'DESCRIPTION_03' column\n",
        "# Removing the leading hyphen\n",
        "data['DESCRIPTION_03'] = data['DESCRIPTION_03'].str.lstrip('-')\n",
        "\n",
        "# Removing trailing spaces\n",
        "data['DESCRIPTION_03'] = data['DESCRIPTION_03'].str.strip()"
      ],
      "metadata": {
        "id": "cPcbX7_7Cas1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Removing trailing spaces for the entire 'DESCRIPTION_02' column\n",
        "data['DESCRIPTION_02'] = data['DESCRIPTION_02'].str.strip()"
      ],
      "metadata": {
        "id": "d9s_cRp9CdIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the unique values and their counts relevant to each categorical column\n",
        "print(\"Unique values and their count relevant to each categorical column:\\n\")\n",
        "for col in categorical_features:\n",
        "    unique_values = data[col].unique()\n",
        "    value_counts = data[col].value_counts()\n",
        "    print(value_counts)\n",
        "    print(\" \")"
      ],
      "metadata": {
        "id": "l9iqBOELChQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Converting all the values in the 'COUNTRY' and the 'UNIT' columns to uppercase values\n",
        "data['COUNTRY'] = data['COUNTRY'].str.upper()\n",
        "data['UNIT'] = data['UNIT'].str.upper()\n",
        "\n",
        "# Displaying the modified dataset\n",
        "print(data)"
      ],
      "metadata": {
        "id": "DTg6jZ6rCkAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Handling the HSCODES\n",
        "\n",
        "# Checking the length of the HSCODES and determining the maximum length\n",
        "data['HSCODE_LENGTH'] = data['HSCODE'].apply(len)\n",
        "max_length = data['HSCODE_LENGTH'].max()\n",
        "\n",
        "# Padding HSCODE values with trailing zeros to match the maximum length\n",
        "data['HSCODE'] = data['HSCODE'].apply(lambda x: x.ljust(max_length, '0'))\n",
        "\n",
        "# Dropping the helper column 'HSCODE_LENGTH' as it's no longer needed\n",
        "data.drop(columns=['HSCODE_LENGTH'], inplace=True)\n",
        "\n",
        "# Displaying the modified dataset\n",
        "print(data)"
      ],
      "metadata": {
        "id": "J97ByA3YCnLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Handling the 'COUNTRY' Column\n",
        "\n",
        "# Replacing specific country names\n",
        "data['COUNTRY'] = data['COUNTRY'].replace({\n",
        "    'United States': 'USA',\n",
        "    'United States of America': 'USA',\n",
        "    'United Kingdom': 'UK',\n",
        "    'Korea, Republic of': 'South Korea'\n",
        "})\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "id": "t4EA9rqHCpgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the unique values and their counts relevant to each categorical column\n",
        "print(\"Unique values and their count relevant to each categorical column:\\n\")\n",
        "for col in categorical_features:\n",
        "    unique_values = data[col].unique()\n",
        "    value_counts = data[col].value_counts()\n",
        "    print(value_counts)\n",
        "    print(\" \")"
      ],
      "metadata": {
        "id": "28TiCorQCs7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Handling Missing Values**"
      ],
      "metadata": {
        "id": "8EkA5UWJQEUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling missing values in the categorical columns**"
      ],
      "metadata": {
        "id": "jyyT1PH2QGni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Replacing all the missing values with 'Unknown'**"
      ],
      "metadata": {
        "id": "7_OnbJJlQJCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 12: Handling missing values in the categorical columns\n",
        "\n",
        "# Replacing all the missing values in the categorical columns with 'Unknown' for imputation\n",
        "data[categorical_features] = data[categorical_features].fillna('Unknown')"
      ],
      "metadata": {
        "id": "18n-W_bbQPId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking for any missing values which is left behind when replacing with 'Unkown'**"
      ],
      "metadata": {
        "id": "_F_Yig3hQSNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifying the changes after replacing the missing values with 'Unknown'\n",
        "print(\"Checking for any missing values left behind after replacing with 'Unknown':\")\n",
        "for col in categorical_features:\n",
        "    null_count = data[col].isnull().sum()\n",
        "    if null_count > 0:\n",
        "        print(f\"{col}: {null_count} missing values\")\n",
        "    else:\n",
        "        print(f\"{col}: No missing values\")\n",
        "    print(\" \")"
      ],
      "metadata": {
        "id": "-UzaNtiQQXWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling the 'Unknown' values in the 'COUNTRY' column using the mode\n",
        "mode_country = data['COUNTRY'].mode()[0]\n",
        "data['COUNTRY'] = data['COUNTRY'].replace('Unknown', mode_country)"
      ],
      "metadata": {
        "id": "IlJAYVg9QaS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping the 'MONTH' and 'IMPORTERS' Columns due to the significant amount of null values\n",
        "data.drop(columns=['MONTH', 'IMPORTER'], inplace=True)"
      ],
      "metadata": {
        "id": "nPLd4_MEQc8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying dataset information\n",
        "print(\"\\nDataset information after categorical imputation:\")\n",
        "data.info()"
      ],
      "metadata": {
        "id": "hZXhr0JhQeIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display unique values in the 'COUNTRY' column\n",
        "unique_countries = data['COUNTRY'].unique()\n",
        "print(\"Unique Countries:\")\n",
        "print(unique_countries)"
      ],
      "metadata": {
        "id": "aG-YBRIAQhHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display unique values in the 'HSCODE' column\n",
        "unique_hscodes = data['HSCODE'].unique()\n",
        "print(\"Unique HSCodes:\")\n",
        "print(unique_hscodes)"
      ],
      "metadata": {
        "id": "TdNAzQj5Qvf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the unique values in the 'UNIT' column\n",
        "unique_units = data['UNIT'].unique()\n",
        "print(\"Unique Units:\")\n",
        "print(unique_units)"
      ],
      "metadata": {
        "id": "wptcjQW_QyAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the unique values in the 'DESCRIPTION_01' column\n",
        "unique_descriptions_01 = data['DESCRIPTION_01'].unique()\n",
        "print(\"Unique Descriptions_01:\")\n",
        "print(unique_descriptions_01)"
      ],
      "metadata": {
        "id": "t5lk4pY8QzQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the unique values in the 'DESCRIPTION_02' column\n",
        "unique_descriptions_02 = data['DESCRIPTION_02'].unique()\n",
        "print(\"Unique Descriptions_02:\")\n",
        "print(unique_descriptions_02)"
      ],
      "metadata": {
        "id": "Xk5F-dOHQ1uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the unique values in the 'DESCRIPTION_03' column\n",
        "unique_descriptions_03 = data['DESCRIPTION_03'].unique()\n",
        "print(\"Unique Descriptions_03:\")\n",
        "print(unique_descriptions_03)"
      ],
      "metadata": {
        "id": "5DesnmIJQ3ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Encoding**"
      ],
      "metadata": {
        "id": "7U6pNi46Q7Gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 13: Encoding the Categorical variables\n",
        "\n",
        "import joblib\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import os\n",
        "\n",
        "# Initialize a dictionary to store encoders\n",
        "encoders = {}\n",
        "\n",
        "# Define the directory where you want to save the encoders\n",
        "encoder_directory = '/content/drive/MyDrive/Importer_Risk_Prediction_2/encoders/'\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(encoder_directory, exist_ok=True)\n",
        "\n",
        "# Encoding categorical columns\n",
        "encoding_columns = ['HSCODE', 'COUNTRY', 'UNIT', 'DESCRIPTION_01', 'DESCRIPTION_02', 'DESCRIPTION_03']\n",
        "for col in encoding_columns:\n",
        "    one_hot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "    encoded = one_hot_encoder.fit_transform(data[[col]])\n",
        "    encoded_df = pd.DataFrame(encoded, columns=[f'{col}_{category}' for category in one_hot_encoder.categories_[0]])\n",
        "    data = pd.concat([data, encoded_df], axis=1)\n",
        "    data.drop(columns=[col], inplace=True)\n",
        "\n",
        "    # Save the encoder for the column with the column name as the file name in the specified directory\n",
        "    encoder_filename = os.path.join(encoder_directory, f'{col}_encoder.pkl')\n",
        "    joblib.dump(one_hot_encoder, encoder_filename)\n",
        "    encoders[col] = one_hot_encoder\n",
        "    print(f\"One-Hot Encoding applied to column: {col} and encoder saved as {encoder_filename}\")\n",
        "\n",
        "print(\"All categorical columns encoded and encoders saved successfully.\")\n"
      ],
      "metadata": {
        "id": "99voN165RpPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Handling missing values in the numerical columns**"
      ],
      "metadata": {
        "id": "YCLc_4uJSASv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using K-NN Imputaion to efficiently capture the small missing values and the captures local patterns in the 'Quantity' coulmn**"
      ],
      "metadata": {
        "id": "IQRfv_yNSCa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 14: Handling missing values in the numerical columns\n",
        "\n",
        "# KNN Imputation for 'QUANTITY'\n",
        "knn_imputer = KNNImputer(n_neighbors=5)\n",
        "data[['QUANTITY']] = knn_imputer.fit_transform(data[['QUANTITY']])"
      ],
      "metadata": {
        "id": "FutjUBGbSFNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 15: Dropping the 'YEAR' n the 'VALUE_RS' Column\n",
        "data.drop(columns=['YEAR', 'VALUE_RS'], inplace=True)"
      ],
      "metadata": {
        "id": "zhLkTaQZSGqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Handling the Outliers**"
      ],
      "metadata": {
        "id": "CX5h1YNNSKaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 16: Handling the outliers in the numerical columns\n",
        "\n",
        "# Assigning the Numerical Columns for the outlier analysis\n",
        "outlier_columns = [\"QUANTITY\"]\n",
        "\n",
        "# Creating an empty dictionary to store the outliers\n",
        "outlier_info = {}\n",
        "\n",
        "for column in outlier_columns:\n",
        "    Q1, Q3 = data[column].quantile([0.25, 0.75])\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Detecting outliers\n",
        "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
        "\n",
        "    outlier_info[column] = {\n",
        "        'Outliers': len(outliers),\n",
        "        'Lower Bound': lower_bound,\n",
        "        'Upper Bound': upper_bound\n",
        "    }\n",
        "\n",
        "    print(f\"Column: {column}\")\n",
        "    print(f\"  Outliers: {outlier_info[column]['Outliers']}, Lower: {lower_bound:.2f}, Upper: {upper_bound:.2f}\")\n",
        "    print(\" \")"
      ],
      "metadata": {
        "id": "x2Kro7E2SOkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visualization of the Outliers**"
      ],
      "metadata": {
        "id": "T2N_o-fkSSqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the outliers for Quantity\n",
        "plt.figure(figsize=(10, 5))\n",
        "Q1 = data['QUANTITY'].quantile(0.25)\n",
        "Q3 = data['QUANTITY'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "sns.boxplot(x=data['QUANTITY'], color='skyblue', flierprops=dict(marker='o', color='red', markersize=5))\n",
        "plt.axvline(lower_bound, color='red', linestyle='--', label='Lower Bound')\n",
        "plt.axvline(upper_bound, color='green', linestyle='--', label='Upper Bound')\n",
        "plt.title('Boxplot of Quantity')\n",
        "plt.xlabel('Quantity')\n",
        "plt.legend()\n",
        "plt.grid(axis='x', alpha=0.75)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P5WydpJRSVKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to handle outliers by capping\n",
        "def handle_outliers(data, method='cap'):\n",
        "    for column in data.columns:\n",
        "      if column == 'QUANTITY':\n",
        "            lower_bound, upper_bound = -0.31, 0.03\n",
        "            data[column] = data[column].apply(lambda x: min(max(x, lower_bound), upper_bound) if method == 'cap' else x)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "# Applying the function to handle outliers\n",
        "data = handle_outliers(data.copy(), method='cap')\n",
        "\n",
        "# Displaying the cleaned dataset\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "2vz9KPWOSYVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 17: Normalizing Quantity\n",
        "scaler = StandardScaler()\n",
        "quantity_scaled = scaler.fit_transform(data[['QUANTITY']])"
      ],
      "metadata": {
        "id": "iVCXzcdNSasa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 18: Normalizing Quantity\n",
        "\n",
        "# Performing K-means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "clusters = kmeans.fit_predict(data)\n",
        "\n",
        "# Assigning risk levels directly\n",
        "importers = np.array(['GlobalChem Corp', 'EcoImports Ltd', 'SafeChem Traders', 'Prime Chemicals Inc', 'ChemTrade Solutions',\n",
        "             'GreenEarth Supplies', 'BioChem Imports', 'Reliable Chemicals Co', 'TransGlobal Imports', 'PureChem Ltd'])\n",
        "\n",
        "data['IMPORTER'] = importers[clusters]\n",
        "\n",
        "print(\"Clustering completed successfully. Importers assigned.\")"
      ],
      "metadata": {
        "id": "51XRepj7ZoC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 19: Encoding the 'IMPORTER' Column\n",
        "\n",
        "# Assuming 'data' is your DataFrame that contains the 'IMPORTER' column\n",
        "one_hot_encoded = pd.get_dummies(data['IMPORTER'], prefix='IMPORTER')\n",
        "\n",
        "# Adding the one-hot encoded columns back to the original DataFrame\n",
        "data = pd.concat([data, one_hot_encoded], axis=1)\n",
        "\n",
        "# Dropping the original 'IMPORTER' column\n",
        "data = data.drop('IMPORTER', axis=1)\n",
        "\n",
        "# Saving the encoder\n",
        "joblib.dump(one_hot_encoded.columns, '/content/drive/MyDrive/Importer_Risk_Prediction_2/encoders/IMPORTER_encoder.pkl')\n",
        "\n",
        "# Displaying the result\n",
        "print(data)"
      ],
      "metadata": {
        "id": "DEaWeuRqaGUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying Dataset Information\n",
        "print(data.info())"
      ],
      "metadata": {
        "id": "1rD7ko8FbQeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying Dataset Head\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "-QuXYxSobWdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Assigning the Target Variable**"
      ],
      "metadata": {
        "id": "JuYcO6eCUX1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 19: Assigning the Target Valriab;\n",
        "\n",
        "# Performing K-means clustering\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "clusters = kmeans.fit_predict(data)\n",
        "\n",
        "# Assigning risk levels directly\n",
        "risk_levels = np.array([\"Low Risk\", \"Moderate Risk\", \"High Risk\", \"No Risk\"])\n",
        "data['RISK'] = risk_levels[clusters]\n",
        "\n",
        "print(\"Clustering completed successfully. Risk levels assigned.\")"
      ],
      "metadata": {
        "id": "IL6C0ejZSeJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display Dataset Information\n",
        "print(data.info())"
      ],
      "metadata": {
        "id": "jWaQ7qyMSjp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Unique Values for All Columns\n",
        "for col in data.columns:\n",
        "    print(f\"Unique values in '{col}':\")\n",
        "    print(data[col].unique())\n",
        "    print('-' * 40)"
      ],
      "metadata": {
        "id": "kDpR3F5ySlok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the Preprocessed Code\n",
        "data.to_csv('/content/drive/MyDrive/Importer_Risk_Prediction_2/Dataset/Preprocessed_before_data.csv', index=False)"
      ],
      "metadata": {
        "id": "04LU5ku_cADD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if 'RISK' column is present\n",
        "if 'RISK' not in data.columns:\n",
        "    raise ValueError(\"Column 'RISK' not found in the dataset\")\n",
        "\n",
        "# Convert RISK column to string type (if not already)\n",
        "data['RISK'] = data['RISK'].astype(str)\n",
        "\n",
        "# Debug: Check unique values\n",
        "print(\"Unique Values in RISK:\", data['RISK'].unique())\n",
        "\n",
        "# Initialize Label Encoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the RISK column\n",
        "data['RISK'] = label_encoder.fit_transform(data['RISK'])\n",
        "\n",
        "# Debug: Check unique encoded labels\n",
        "print(\"Encoded Labels:\", label_encoder.classes_)\n",
        "\n",
        "# Save the Encoder using joblib\n",
        "joblib.dump(label_encoder, '/content/drive/MyDrive/Importer_Risk_Prediction_2/encoders/label_encoder.pkl')\n",
        "print(\"Label Encoder model saved as 'label_encoder.pkl'\")\n",
        "\n",
        "# Preview Final Data\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "KcCQh34TSoRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the Dataset head\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "VNR-okz4cQGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Style\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='RISK', data=data, palette='viridis')\n",
        "plt.title('Target Variable Distribution')\n",
        "plt.xlabel('Risk Category')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fxtA0Xa2SqE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Features (X) and Target (y)\n",
        "X = data.drop(['RISK'], axis=1)\n",
        "y = data['RISK']"
      ],
      "metadata": {
        "id": "nN6JBsU0Ssj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the Columns in X\n",
        "print(\"Columns in X:\")\n",
        "print(X.columns)"
      ],
      "metadata": {
        "id": "84qwIwfzksv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting dataset into training and testing before SMOTE\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "print(\"Training Set Size:\", X_train.shape[0])\n",
        "print(\"Testing Set Size:\", X_test.shape[0])"
      ],
      "metadata": {
        "id": "9DHyMS-NSuDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Balancing the Target Variable**"
      ],
      "metadata": {
        "id": "0JHK-_x6Sw4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling the imbalance dataset\n",
        "\n",
        "# Suppressing the FutureWarnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Handling imbalanced dataset\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "print(\"Original dataset size:\", X.shape)\n",
        "print(\"Resampled dataset size:\", X_resampled.shape)"
      ],
      "metadata": {
        "id": "g6VDm2-6SzZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting class distribution after SMOTE\n",
        "sns.countplot(x=y_resampled, palette=\"viridis\")\n",
        "plt.title(\"Class Distribution After SMOTE\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P7Zrl0HrS0lU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the dataset shape after applying the SMOTE\n",
        "print(f\"Shape of X_resampled: {X_resampled.shape}\")\n",
        "print(f\"Shape of y_resampled: {y_resampled.shape}\")"
      ],
      "metadata": {
        "id": "uo2Ip6FAS4ED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the Columns in X\n",
        "print(\"Columns in X:\")\n",
        "print(X_resampled.columns)"
      ],
      "metadata": {
        "id": "BjR44SyBlgcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardizing the features\n",
        "scaler = StandardScaler()\n",
        "X_resampled_scaled = scaler.fit_transform(X_resampled)"
      ],
      "metadata": {
        "id": "YU1fxB_hS7ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled_scaled, y_resampled, test_size=0.2, stratify = y_resampled, random_state=42)"
      ],
      "metadata": {
        "id": "g41iPPK5S8t7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the Dataset Information\n",
        "print(\"Dataset Information:\")\n",
        "print(data.info())"
      ],
      "metadata": {
        "id": "nZATsR0YS-ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the Preprocessed Code\n",
        "data.to_csv('/content/drive/MyDrive/Importer_Risk_Prediction_2/Dataset/Preprocessed_data.csv', index=False)"
      ],
      "metadata": {
        "id": "EjaGfhw_Veuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Training**"
      ],
      "metadata": {
        "id": "N1Za5KdFTEne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the RandomForestClassifier (without hyperparameter tuning)\n",
        "rf = RandomForestClassifier(random_state=42)"
      ],
      "metadata": {
        "id": "BWwxFj7fTIwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the RandomForest model on the training data\n",
        "rf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "g32YIMtETKBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test set\n",
        "y_pred = rf.predict(X_test)"
      ],
      "metadata": {
        "id": "ztA_4F3ETLJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(f\"Confusion Matrix:\\n{cm}\")"
      ],
      "metadata": {
        "id": "cnsWFMQxTOW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Confusion Matrix\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_resampled), yticklabels=np.unique(y_resampled))\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9066f00rTRT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(f\"Classification Report:\\n{report}\")"
      ],
      "metadata": {
        "id": "Y_BQ4HEMTVZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy score on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy on test set: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "r0eH-e0aTZLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the Random Forest Model\n",
        "joblib.dump(rf, '/content/drive/MyDrive/Importer_Risk_Prediction_2/models/random_forest_model.joblib')\n",
        "print(\"Random Forest Model Saved Successfully!\")"
      ],
      "metadata": {
        "id": "UEMDHJmVTffe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The Predictions**"
      ],
      "metadata": {
        "id": "LJOwoulykDX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "\n",
        "# Load trained model\n",
        "try:\n",
        "    model = joblib.load('/content/drive/MyDrive/Importer_Risk_Prediction_2/models/random_forest_model.joblib')\n",
        "    print(\"Model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Load LabelEncoder for target variable\n",
        "try:\n",
        "    label_encoder = joblib.load('/content/drive/MyDrive/Importer_Risk_Prediction_2/encoders/target_label_encoder.pkl')\n",
        "    print(\"Target LabelEncoder loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading target LabelEncoder: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Define categorical columns\n",
        "categorical_columns = ['IMPORTER', 'HSCODE', 'COUNTRY', 'UNIT', 'DESCRIPTION_01', 'DESCRIPTION_02', 'DESCRIPTION_03']\n",
        "\n",
        "# Load OneHotEncoders for each categorical column\n",
        "encoders = {}\n",
        "for col in categorical_columns:\n",
        "    try:\n",
        "        encoder_file = f'/content/drive/MyDrive/Importer_Risk_Prediction_2/encoders/{col}_encoder.pkl'\n",
        "        encoders[col] = joblib.load(encoder_file)\n",
        "        print(f\"{col} encoder loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading encoder for {col}: {e}\")\n",
        "        exit()\n",
        "\n",
        "def get_user_input():\n",
        "    \"\"\"Get user input for the prediction.\"\"\"\n",
        "    print(\"\\nPlease enter the following details:\")\n",
        "    importer = input(\"Enter Importer: \")\n",
        "    hscode = input(\"Enter HS Code: \")\n",
        "    country = input(\"Enter Country: \")\n",
        "    unit = input(\"Enter Unit: \")\n",
        "    description_01 = input(\"Enter Description 1: \")\n",
        "    description_02 = input(\"Enter Description 2: \")\n",
        "    description_03 = input(\"Enter Description 3: \")\n",
        "\n",
        "    try:\n",
        "        quantity = float(input(\"Enter Quantity: \"))\n",
        "    except ValueError:\n",
        "        print(\"Invalid quantity. Please enter a numeric value.\")\n",
        "        exit()\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'IMPORTER': [importer],\n",
        "        'HSCODE': [hscode],\n",
        "        'COUNTRY': [country],\n",
        "        'UNIT': [unit],\n",
        "        'DESCRIPTION_01': [description_01],\n",
        "        'DESCRIPTION_02': [description_02],\n",
        "        'DESCRIPTION_03': [description_03],\n",
        "        'QUANTITY': [quantity]\n",
        "    })\n",
        "\n",
        "def preprocess_input(data):\n",
        "    \"\"\"Preprocess user input using saved OneHotEncoders.\"\"\"\n",
        "    try:\n",
        "        # Prepare the encoded categorical data\n",
        "        encoded_features = []\n",
        "\n",
        "        for col in categorical_columns:\n",
        "            encoder = encoders.get(col)\n",
        "            if encoder:\n",
        "                # Transform the input and convert to array if needed\n",
        "                encoded = encoder.transform(data[[col]])\n",
        "                if hasattr(encoded, \"toarray\"):\n",
        "                    encoded = encoded.toarray()\n",
        "                encoded_features.append(encoded)\n",
        "            else:\n",
        "                print(f\"Encoder not found for column: {col}\")\n",
        "                exit()\n",
        "\n",
        "        # Convert the list of encoded features to a single numpy array\n",
        "        encoded_features = np.hstack(encoded_features)\n",
        "\n",
        "        # Keep numerical features (quantity)\n",
        "        numerical_features = data[['QUANTITY']].values\n",
        "\n",
        "        # Combine encoded categorical features with numerical features\n",
        "        final_features = np.hstack((encoded_features, numerical_features))\n",
        "\n",
        "        return final_features\n",
        "    except Exception as e:\n",
        "        print(f\"Error during preprocessing: {e}\")\n",
        "        exit()\n",
        "\n",
        "def predict_risk():\n",
        "    \"\"\"Predict risk level for user input.\"\"\"\n",
        "    user_data = get_user_input()\n",
        "    processed_data = preprocess_input(user_data)\n",
        "\n",
        "    try:\n",
        "        # Ensure the input is 2D for the model (shape should be (1, n_features))\n",
        "        processed_data = np.array(processed_data).reshape(1, -1)\n",
        "\n",
        "        # Make the prediction (numeric output)\n",
        "        risk_prediction_numeric = model.predict(processed_data)[0]\n",
        "\n",
        "        # Convert numerical prediction back to original label\n",
        "        risk_prediction_label = label_encoder.inverse_transform([risk_prediction_numeric])[0]\n",
        "\n",
        "        print(f\"Predicted Risk Level: {risk_prediction_label}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during prediction: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    predict_risk()\n"
      ],
      "metadata": {
        "id": "nDtMO03okHQf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}