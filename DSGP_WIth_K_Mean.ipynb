{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxxPC0395tpQNy+5g74gQc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShabnaIlmi/Data-Science-Group-Project/blob/Future_Risk_prediction/DSGP_WIth_K_Mean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfuQrxgMuF7m"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import IterativeImputer, KNNImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "import warnings\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import KNNImputer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten\n",
        "import seaborn as sns\n",
        "import random\n",
        "import pickle\n",
        "import joblib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_QDVwATOGOiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mmount google drive with colab\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#loading the Dataset\n",
        "\n",
        "df = pd.read_excel('/content/drive/MyDrive/DSGP/IMPORT STATISTICS - 2023.xlsx')"
      ],
      "metadata": {
        "id": "l3CdjmILuLaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the dataset\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "QG4wO0zduLdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#displaying the dataset information\n",
        "df.info()"
      ],
      "metadata": {
        "id": "G58UpDrGuLiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying the statistical summary of dataset\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "bqfVgHfbuLlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identifying categorical and numerical columns\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Displaying the Categorical and the Numerical columns'\n",
        "print(\"Categorical Columns:\")\n",
        "print(categorical_cols)\n",
        "\n",
        "print(\"\\nNumerical Columns:\")\n",
        "print(numerical_cols)"
      ],
      "metadata": {
        "id": "i9Zr-sVBuLqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting 'IMPORTER' and 'HSCODE' columns to string type\n",
        "df['HSCODE'] = df['HSCODE'].astype(str)\n",
        "df['IMPORTER'] = df['IMPORTER'].astype(str)\n",
        "\n",
        "# Displaying the HSCODE column data type\n",
        "print(\"Data Type of HSCODE Columns:\")\n",
        "print(df[['HSCODE']].dtypes)\n",
        "\n",
        "print(\"Data Type of IMPORTER Columns:\")\n",
        "print(df[['IMPORTER']].dtypes)"
      ],
      "metadata": {
        "id": "nwWXmot0uLtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Displaying the categorical features\n",
        "print(\"Categorical Features:\")\n",
        "for feature in categorical_cols:\n",
        "    print(f\"- {feature}\")\n",
        "\n",
        "# Display data type of the columns\n",
        "print(\"\\nData Type of Categorical Features:\")\n",
        "print(df[categorical_cols].dtypes)"
      ],
      "metadata": {
        "id": "_g_KaH0BuLv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analyzing the cetagorical varibles\n",
        "# Bar plots for categorical columns\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.subplot(2, 2, 1)\n",
        "sns.countplot(y=df['COUNTRY'], order=df['COUNTRY'].value_counts().index)\n",
        "plt.title('Distribution of COUNTRY')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "sns.countplot(y=df['MONTH'], order=df['MONTH'].value_counts().index)\n",
        "plt.title('Distribution of MONTH')\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "sns.countplot(y=df['UNIT'], order=df['UNIT'].value_counts().index)\n",
        "plt.title('Distribution of UNIT')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_EK9TGENuLyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the unique values and their counts relevant to each categorical column\n",
        "print(\"Unique values and their count relevant to each categorical column:\\n\")\n",
        "for col in categorical_cols:\n",
        "    unique_values = df[col].unique()\n",
        "    value_counts = df[col].value_counts()\n",
        "    print(value_counts)\n",
        "    print(\" \")"
      ],
      "metadata": {
        "id": "U62-ViXxuL1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Missing value --------------------------------"
      ],
      "metadata": {
        "id": "CQDMwazavixU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the categorical columns which contains null values and their counts\n",
        "print(\"Categorical columns with null values and their counts:\")\n",
        "for col in categorical_cols:\n",
        "    null_count = df[col].isnull().sum()\n",
        "    if null_count > 0:\n",
        "        print(f\"{col}: {null_count}\")"
      ],
      "metadata": {
        "id": "YO8OI_c-uL37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the categorical columns which contain 'Unknown' values and their relevant counts\n",
        "print(\"Categorical columns with 'Unknown' values and their counts:\")\n",
        "for col in categorical_cols:\n",
        "    unknown_count = (df[col] == 'Unknown').sum()\n",
        "    if unknown_count > 0:\n",
        "        print(f\"{col}: {unknown_count}\")"
      ],
      "metadata": {
        "id": "BHe1bFdSviQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of numerical features\n",
        "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Displaying the numerical features\n",
        "print(\"Numerical Features:\")\n",
        "for feature in numerical_cols:\n",
        "    print(f\"- {feature}\")\n",
        "\n",
        "# Display the data type of the numerical features\n",
        "print(\"\\nData Type of Numerical Features:\")\n",
        "print(df[numerical_cols].dtypes)"
      ],
      "metadata": {
        "id": "ta0-_6mCuL6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the unique values and their count in the numerical columns\n",
        "print(\"Unique values and their count in the numerical columns:\\n\")\n",
        "for col in numerical_cols:\n",
        "    unique_values = df[col].unique()\n",
        "    value_counts = df[col].value_counts()\n",
        "    print(value_counts)\n",
        "    print(\" \")"
      ],
      "metadata": {
        "id": "8mc4vT-BuL87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking data disstrubition\n",
        "# Plot histograms for numerical columns\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df['QUANTITY'], kde=True, bins=30)\n",
        "plt.title('Distribution of QUANTITY')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(df['VALUE_RS'], kde=True, bins=30)\n",
        "plt.title('Distribution of VALUE_RS')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FFqOvFk0uL_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for Outliers\n",
        "# Box plots for numerical columns\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.boxplot(y=df['QUANTITY'])\n",
        "plt.title('Box Plot of QUANTITY')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(y=df['VALUE_RS'])\n",
        "plt.title('Box Plot of VALUE_RS')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CVziwHmUv-8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check Correlation Between Variables\n",
        "# Correlation heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(df[['QUANTITY', 'VALUE_RS']].corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4txZ62Ssv-_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the numerical columns with null values and their relevant counts\n",
        "print(\"Numerical columns with null values and their relevant counts:\")\n",
        "for col in numerical_cols:\n",
        "    null_count = df[col].isnull().sum()\n",
        "    if null_count > 0:\n",
        "        print(f\"{col}: {null_count}\")"
      ],
      "metadata": {
        "id": "rigHH-tCv_Cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocesssing\n"
      ],
      "metadata": {
        "id": "izvVL8knw6mV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing whitespaces from the object type columns\n",
        "object_columns = df.select_dtypes(include=['object']).columns\n",
        "df[object_columns] = df[object_columns].apply(lambda x: x.str.strip())"
      ],
      "metadata": {
        "id": "NzuCEox6v_KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing unnecessary full stops(\".\") from the categorical columns\n",
        "df[categorical_cols] = df[categorical_cols].apply(lambda x: x.str.replace('.', ''))"
      ],
      "metadata": {
        "id": "Foz-Tpyfv_N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Removing unnecessary special characters and trailing spaces from the 'DESCRIPTION_03' column\n",
        "# Removing the leading hyphen\n",
        "df['DESCRIPTION_03'] = df['DESCRIPTION_03'].str.lstrip('-')\n",
        "\n",
        "# Removing trailing spaces\n",
        "df['DESCRIPTION_03'] = df['DESCRIPTION_03'].str.strip()"
      ],
      "metadata": {
        "id": "uH55axR3v_RE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting all the values in the 'COUNTRY' and the 'UNIT' columns to uppercase values\n",
        "df['COUNTRY'] = df['COUNTRY'].str.upper()\n",
        "df['UNIT'] = df['UNIT'].str.upper()\n",
        "\n",
        "# Displaying the modified dataset\n",
        "print(df)"
      ],
      "metadata": {
        "id": "nXWfw3HXv_T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing trailing spaces for the entire 'DESCRIPTION_02' column\n",
        "df['DESCRIPTION_02'] = df['DESCRIPTION_02'].str.strip()"
      ],
      "metadata": {
        "id": "txMVKqJJv_Wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the unique values and their counts relevant to each categorical column\n",
        "print(\"Unique values and their count relevant to each categorical column:\\n\")\n",
        "for col in categorical_cols:\n",
        "    unique_values = df[col].unique()\n",
        "    value_counts = df[col].value_counts()\n",
        "    print(value_counts)\n",
        "    print(\" \")"
      ],
      "metadata": {
        "id": "INplrl-Ev_bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting all the values in the 'COUNTRY' and the 'UNIT' columns to uppercase values\n",
        "df['COUNTRY'] = df['COUNTRY'].str.upper()\n",
        "df['UNIT'] = df['UNIT'].str.upper()\n",
        "\n",
        "# Displaying the modified dataset\n",
        "print(df)"
      ],
      "metadata": {
        "id": "tgAc_hNOy6qG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ub9lGYyiv_es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Handling the HSCODES\n",
        "\n",
        "# Checking the length of the HSCODES and determining the maximum length\n",
        "df['HSCODE_LENGTH'] = df['HSCODE'].apply(len)\n",
        "max_length = df['HSCODE_LENGTH'].max()\n",
        "\n",
        "# Padding HSCODE values with trailing zeros to match the maximum length\n",
        "df['HSCODE'] = df['HSCODE'].apply(lambda x: x.ljust(max_length, '0'))\n",
        "\n",
        "# Dropping the helper column 'HSCODE_LENGTH' as it's no longer needed\n",
        "df.drop(columns=['HSCODE_LENGTH'], inplace=True)\n",
        "\n",
        "# Displaying the modified dataset\n",
        "print(df)\n",
        ""
      ],
      "metadata": {
        "id": "p2IdMGaPv_hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Handling the 'COUNTRY' Column\n",
        "\n",
        "# Replacing specific country names\n",
        "df['COUNTRY'] = df['COUNTRY'].replace({\n",
        "    'United States': 'USA',\n",
        "    'United States of America': 'USA',\n",
        "    'United Kingdom': 'UK',\n",
        "    'Korea, Republic of': 'South Korea'\n",
        "})\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "HDdyUnCJv_mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the unique values and their counts relevant to each categorical column\n",
        "print(\"Unique values and their count relevant to each categorical column:\\n\")\n",
        "for col in categorical_cols:\n",
        "    unique_values = df[col].unique()\n",
        "    value_counts = df[col].value_counts()\n",
        "    print(value_counts)\n",
        "    print(\" \")"
      ],
      "metadata": {
        "id": "Hv_RO4KFv_pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Missing Values"
      ],
      "metadata": {
        "id": "Iqx11S3a46Oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Handling missing values in the categorical columns\n",
        "\n",
        "# Replacing all the missing values in the categorical columns with 'Unknown' for imputation\n",
        "df[categorical_cols] = df[categorical_cols].fillna('Unknown')"
      ],
      "metadata": {
        "id": "2WQLw2Whv_s8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifying the changes after replacing the missing values with 'Unknown'\n",
        "print(\"Checking for any missing values left behind after replacing with 'Unknown':\")\n",
        "for col in categorical_cols:\n",
        "    null_count = df[col].isnull().sum()\n",
        "    if null_count > 0:\n",
        "        print(f\"{col}: {null_count} missing values\")\n",
        "    else:\n",
        "        print(f\"{col}: No missing values\")\n",
        "    print(\" \")"
      ],
      "metadata": {
        "id": "hKC2UcmZv_ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling the 'Unknown' values in the 'COUNTRY' column using the mode\n",
        "mode_country = df['COUNTRY'].mode()[0]\n",
        "df['COUNTRY'] = df['COUNTRY'].replace('Unknown', mode_country)"
      ],
      "metadata": {
        "id": "RBeQvZYrv_2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping the 'MONTH' and 'IMPORTERS' Columns due to the significant amount of null values\n",
        "df.drop(columns=['MONTH', 'IMPORTER'], inplace=True)"
      ],
      "metadata": {
        "id": "W8fRTjT6v_5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying dataset information\n",
        "print(\"\\nDataset information after categorical imputation:\")\n",
        "df.info()"
      ],
      "metadata": {
        "id": "VSb1oLKZv_-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display unique values in the 'COUNTRY' column\n",
        "unique_countries = df['COUNTRY'].unique()\n",
        "print(\"Unique Countries:\")\n",
        "print(unique_countries)"
      ],
      "metadata": {
        "id": "I-orf1Ua6lrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display unique values in the 'HSCODE' column\n",
        "unique_hscodes = df['HSCODE'].unique()\n",
        "print(\"Unique HSCodes:\")\n",
        "print(unique_hscodes)"
      ],
      "metadata": {
        "id": "6w6Rth2R6lt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the unique values in the 'UNIT' column\n",
        "unique_units = df['UNIT'].unique()\n",
        "print(\"Unique Units:\")\n",
        "print(unique_units)"
      ],
      "metadata": {
        "id": "GqRRQz196lw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the unique values in the 'DESCRIPTION_01' column\n",
        "unique_descriptions_01 = df['DESCRIPTION_01'].unique()\n",
        "print(\"Unique Descriptions_01:\")\n",
        "print(unique_descriptions_01)"
      ],
      "metadata": {
        "id": "WIMVowqJ6lzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the unique values in the 'DESCRIPTION_02' column\n",
        "unique_descriptions_02 = df['DESCRIPTION_02'].unique()\n",
        "print(\"Unique Descriptions_02:\")\n",
        "print(unique_descriptions_02)"
      ],
      "metadata": {
        "id": "oTJgEBE66l2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Displaying the unique values in the 'DESCRIPTION_03' column\n",
        "unique_descriptions_03 = df['DESCRIPTION_03'].unique()\n",
        "print(\"Unique Descriptions_03:\")\n",
        "print(unique_descriptions_03)"
      ],
      "metadata": {
        "id": "IZdR_gml6l4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Encoding"
      ],
      "metadata": {
        "id": "v0PhjZaL8lcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 11: Encoding the Categorical Columns\n",
        "encoding_columns = ['HSCODE', 'COUNTRY', 'UNIT', 'DESCRIPTION_01', 'DESCRIPTION_02', 'DESCRIPTION_03']\n",
        "\n",
        "# Loop through the columns to apply One-Hot Encoding\n",
        "for col in encoding_columns:\n",
        "    one_hot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "    encoded = one_hot_encoder.fit_transform(df[[col]])\n",
        "    encoded_df = pd.DataFrame(encoded, columns=[f'{col}_{category}' for category in one_hot_encoder.categories_[0]])  # Convert to DataFrame\n",
        "    df = pd.concat([df, encoded_df], axis=1)  # Concatenate the encoded DataFrame\n",
        "    df.drop(columns=[col], inplace=True)  # Drop the original categorical column\n",
        "    print(f\"One-Hot Encoding applied to column: {col}\")\n",
        "\n",
        "print(\"All categorical columns encoded successfully.\")\n",
        ""
      ],
      "metadata": {
        "id": "rCZ7wAk46l6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "W2shX7ue6l9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling missing values in the numerical columns"
      ],
      "metadata": {
        "id": "fYjNI6lF91M-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling missing values in the numerical columns\n",
        "\n",
        "# KNN Imputation for 'QUANTITY'\n",
        "knn_imputer = KNNImputer(n_neighbors=5)\n",
        "df[['QUANTITY']] = knn_imputer.fit_transform(df[['QUANTITY']])"
      ],
      "metadata": {
        "id": "OYF5qmHV6mCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Dropping the 'YEAR' n the 'VALUE_RS' Column\n",
        "df.drop(columns=['YEAR', 'VALUE_RS'], inplace=True)"
      ],
      "metadata": {
        "id": "_XjTAJ-D912-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#displaying the dataset information\n",
        "df.info()"
      ],
      "metadata": {
        "id": "471Bd1ga9199"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n"
      ],
      "metadata": {
        "id": "oiPQlAEO_Npt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling the Outliers"
      ],
      "metadata": {
        "id": "oj5G8n-c_dz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for Outliers\n",
        "# Box plots for numerical columns\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.boxplot(y=df['QUANTITY'])\n",
        "plt.title('Box Plot of QUANTITY')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BvVwHC76_TU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling the outliers in the numerical columns\n",
        "\n",
        "# Assigning the Numerical Columns for the outlier analysis\n",
        "outlier_columns = [\"QUANTITY\"]\n",
        "\n",
        "# Creating an empty dictionary to store the outliers\n",
        "outlier_info = {}\n",
        "\n",
        "for column in outlier_columns:\n",
        "    Q1, Q3 = df[column].quantile([0.25, 0.75])\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Detecting outliers\n",
        "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
        "\n",
        "    outlier_info[column] = {\n",
        "        'Outliers': len(outliers),\n",
        "        'Lower Bound': lower_bound,\n",
        "        'Upper Bound': upper_bound\n",
        "    }\n",
        "\n",
        "    print(f\"Column: {column}\")\n",
        "    print(f\"  Outliers: {outlier_info[column]['Outliers']}, Lower: {lower_bound:.2f}, Upper: {upper_bound:.2f}\")\n",
        "    print(\" \")\n",
        ""
      ],
      "metadata": {
        "id": "3HFipJw8_TZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the outliers for Quantity\n",
        "plt.figure(figsize=(10, 5))\n",
        "Q1 = df['QUANTITY'].quantile(0.25)\n",
        "Q3 = df['QUANTITY'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "sns.boxplot(x=df['QUANTITY'], color='skyblue', flierprops=dict(marker='o', color='red', markersize=5))\n",
        "plt.axvline(lower_bound, color='red', linestyle='--', label='Lower Bound')\n",
        "plt.axvline(upper_bound, color='green', linestyle='--', label='Upper Bound')\n",
        "plt.title('Boxplot of Quantity')\n",
        "plt.xlabel('Quantity')\n",
        "plt.legend()\n",
        "plt.grid(axis='x', alpha=0.75)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZHHrRpMw_Tcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to handle outliers by capping\n",
        "def handle_outliers(df, method='cap'):\n",
        "    for column in df.columns:\n",
        "      if column == 'QUANTITY':\n",
        "            lower_bound, upper_bound = -0.31, 0.03\n",
        "            df[column] = df[column].apply(lambda x: min(max(x, lower_bound), upper_bound) if method == 'cap' else x)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Applying the function to handle outliers\n",
        "df = handle_outliers(df.copy(), method='cap')\n",
        "\n",
        "# Displaying the cleaned dataset\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "OfXAdQGh_Thl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying K-mean clustering"
      ],
      "metadata": {
        "id": "tzD5duJhBgSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clustering using K-means\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "df['Cluster'] = kmeans.fit_predict(df)\n",
        "print(\"Clustering completed successfully.\")"
      ],
      "metadata": {
        "id": "4T_ijWRC_Tkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 20: Assign Risk Categories\n",
        "risk_map = {0: 'Low Risk', 1: 'Medium Risk', 2: 'High Risk'}\n",
        "df['RISK'] = df['Cluster'].map(risk_map)\n",
        ""
      ],
      "metadata": {
        "id": "Q67MBbsj_Tm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping the Cluster Column\n",
        "df.drop('Cluster', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "f2RH-xf2_TrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display Dataset Information\n",
        "print(df.info())"
      ],
      "metadata": {
        "id": "-BZ3NIEC_Tt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "9qQY9SS2_TzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Unique Values for All Columns\n",
        "for col in df.columns:\n",
        "    print(f\"Unique values in '{col}':\")\n",
        "    print(df[col].unique())\n",
        "    print('-' * 40)"
      ],
      "metadata": {
        "id": "IoRs9hX0_T2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode Target Variable (Risk)\n",
        "target_encoder = LabelEncoder()\n",
        "df['RISK'] = target_encoder.fit_transform(df['RISK'])\n",
        "\n",
        "# Print Encoded Values\n",
        "print(\"Original Risk Categories:\")\n",
        "print(df['RISK'].unique())\n",
        "print(\"\\nEncoded Risk Categories:\")\n",
        "print(df['RISK'].unique())"
      ],
      "metadata": {
        "id": "rgQp-XU4_T5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set Style\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='RISK', data=df, palette='viridis')\n",
        "plt.title('Target Variable Distribution')\n",
        "plt.xlabel('Risk Category')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vCBYEtz6_T9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the occurrences of each risk category\n",
        "risk_counts = df['RISK'].value_counts()\n",
        "\n",
        "# Print the counts for each risk category\n",
        "print(\"Risk Category Counts:\")\n",
        "print(risk_counts)\n"
      ],
      "metadata": {
        "id": "9A9uZL5c_UAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Features (X) and Target (y)\n",
        "X = df.drop(['RISK'], axis=1)\n",
        "y = df['RISK']\n",
        ""
      ],
      "metadata": {
        "id": "WGJklBY1DvBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Splitting dataset into training and testing before SMOTE\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "print(\"Training Set Size:\", X_train.shape[0])\n",
        "print(\"Testing Set Size:\", X_test.shape[0])"
      ],
      "metadata": {
        "id": "kc1ERW3PDvD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling class Imbalance"
      ],
      "metadata": {
        "id": "50cDiFwUGQ0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling the imbalance dataset\n",
        "\n",
        "# Suppressing the FutureWarnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Handling imbalanced dataset\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "print(\"Original dataset size:\", X.shape)\n",
        "print(\"Resampled dataset size:\", X_resampled.shape)"
      ],
      "metadata": {
        "id": "BEWInhJvDvI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting class distribution after SMOTE\n",
        "sns.countplot(x=y_resampled, palette=\"viridis\")\n",
        "plt.title(\"Class Distribution After SMOTE\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hhXmPxhBDvL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the dataset shape after applying the SMOTE\n",
        "print(f\"Shape of X_resampled: {X_resampled.shape}\")\n",
        "print(f\"Shape of y_resampled: {y_resampled.shape}\")"
      ],
      "metadata": {
        "id": "s-FNUeW4Gzjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardizing the features\n",
        "scaler = StandardScaler()\n",
        "X_resampled_scaled = scaler.fit_transform(X_resampled)"
      ],
      "metadata": {
        "id": "L6e63sVlGzme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled_scaled, y_resampled, test_size=0.2, stratify = y_resampled, random_state=42)"
      ],
      "metadata": {
        "id": "h9D8QJUdGzqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the Dataset Information\n",
        "print(\"Dataset Information:\")\n",
        "print(df.info())"
      ],
      "metadata": {
        "id": "eLhhlCZCGzu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Model"
      ],
      "metadata": {
        "id": "3H1vdXmiLfzi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Based on Random Forest"
      ],
      "metadata": {
        "id": "_Q3B4BAgLjWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the RandomForestClassifier (without hyperparameter tuning)\n",
        "rf = RandomForestClassifier(random_state=24)"
      ],
      "metadata": {
        "id": "APBP9c6sGzyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the RandomForest model on the training data\n",
        "rf.fit(X_train, y_train)\n",
        ""
      ],
      "metadata": {
        "id": "7lk6nX02Gz1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Predict on the test set\n",
        "y_pred = rf.predict(X_test)"
      ],
      "metadata": {
        "id": "HR7cm2H6Gz3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(f\"Confusion Matrix:\\n{cm}\")"
      ],
      "metadata": {
        "id": "H76-IZBHGz54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Confusion Matrix\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_resampled), yticklabels=np.unique(y_resampled))\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-evVow1AGz8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(f\"Classification Report:\\n{report}\")"
      ],
      "metadata": {
        "id": "WKW9NFnbGz-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy score on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy on test set: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "_sUoTv9PG0Fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "class ChemicalRiskPredictor:\n",
        "    def __init__(self,\n",
        "                 model_path='/content/drive/MyDrive/Importer_Risk_Prediction_2/models/',\n",
        "                 scaler_path='/content/drive/MyDrive/Importer_Risk_Prediction_2/scalers/'):\n",
        "        \"\"\"\n",
        "        Initialize the predictor by loading necessary components\n",
        "        \"\"\"\n",
        "        # Load scaler and Random Forest model\n",
        "        self.scaler = joblib.load(f'{scaler_path}scaler.joblib')\n",
        "        self.rf_model = joblib.load(f'{model_path}random_forest_model.joblib')\n",
        "\n",
        "        # Initialize OneHotEncoder and the pipeline\n",
        "        self.preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('cat', OneHotEncoder(), ['COUNTRY', 'UNIT', 'HSCODE', 'DESCRIPTION_01', 'DESCRIPTION_02', 'DESCRIPTION_03']),\n",
        "                ('num', StandardScaler(), ['QUANTITY'])  # Assuming QUANTITY is the only numeric feature\n",
        "            ])\n",
        "\n",
        "    def predict(self, input_data):\n",
        "        \"\"\"\n",
        "        Make prediction using the Random Forest model with user input data\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Prepare features from input_data\n",
        "            features = [\n",
        "                input_data['QUANTITY'],  # Numeric feature\n",
        "                input_data['HSCODE'],    # Categorical feature\n",
        "                input_data['COUNTRY'],   # Categorical feature\n",
        "                input_data['UNIT'],      # Categorical feature\n",
        "                input_data['DESCRIPTION_01'], # Categorical feature\n",
        "                input_data['DESCRIPTION_02'], # Categorical feature\n",
        "                input_data['DESCRIPTION_03'], # Categorical feature\n",
        "            ]\n",
        "\n",
        "            # Convert to numpy array and reshape\n",
        "            features = np.array(features).reshape(1, -1)\n",
        "\n",
        "            # Apply preprocessing (One-Hot Encoding + Scaling)\n",
        "            processed_features = self.preprocessor.transform(features)\n",
        "\n",
        "            # Make prediction\n",
        "            prediction = self.rf_model.predict(processed_features)\n",
        "            prediction_proba = self.rf_model.predict_proba(processed_features)\n",
        "\n",
        "            # Determine risk category\n",
        "            risk_categories = ['Low', 'Medium', 'High']\n",
        "            risk_index = np.argmax(prediction_proba)\n",
        "            risk_category = risk_categories[risk_index]\n",
        "\n",
        "            return {\n",
        "                'risk_category': risk_category,\n",
        "                'confidence': prediction_proba[0][risk_index],\n",
        "                'detailed_probabilities': {\n",
        "                    category: prob for category, prob in zip(risk_categories, prediction_proba[0])\n",
        "                }\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Prediction error: {e}\")\n",
        "            return None\n",
        "\n",
        "def main():\n",
        "    # Accept values directly during input\n",
        "    try:\n",
        "        input_data = {\n",
        "            'COUNTRY': input(\"Enter country: \").upper(),\n",
        "            'UNIT': input(\"Enter unit: \").upper(),\n",
        "            'HSCODE': input(\"Enter HS code: \").strip(),\n",
        "            'DESCRIPTION_01': input(\"Enter description 01: \").capitalize(),\n",
        "            'DESCRIPTION_02': input(\"Enter description 02: \").capitalize(),\n",
        "            'DESCRIPTION_03': input(\"Enter description 03: \").capitalize(),\n",
        "            'QUANTITY': float(input(\"Enter quantity: \")),\n",
        "        }\n",
        "\n",
        "        # Create predictor instance\n",
        "        predictor = ChemicalRiskPredictor()\n",
        "\n",
        "        # Make prediction\n",
        "        result = predictor.predict(input_data)\n",
        "\n",
        "        # Display results\n",
        "        if result:\n",
        "            print(\"\\nPrediction Results:\")\n",
        "            print(f\"Risk Category: {result['risk_category']}\")\n",
        "            print(f\"Confidence: {result['confidence']:.2%}\")\n",
        "            print(\"\\nDetailed Probabilities:\")\n",
        "            for category, probability in result['detailed_probabilities'].items():\n",
        "                print(f\"{category} Risk: {probability:.2%}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in prediction: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SGWKR_9G0L-",
        "outputId": "638b671e-bc6a-4fa0-8875-e611a500aa57"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter HS code: 23233233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Features (X) and Target (y)\n",
        "X = df.drop(['RISK'], axis=1)\n",
        "y = df['RISK']\n",
        "\n",
        "# Splitting data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Handle class imbalance with SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_resampled), y=y_train_resampled)\n",
        "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "# Define the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_shape=(X_train_resampled.shape[1],)))\n",
        "model.add(Dropout(0.3))  # Dropout for regularization\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(3, activation='softmax'))  # Output layer (3 risk categories)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Add early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_resampled, y_train_resampled,\n",
        "                    epochs=50, batch_size=32, validation_split=0.1,\n",
        "                    class_weight=class_weight_dict, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_prob = model.predict(X_test_scaled)\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "# Print metrics\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2%}\")\n",
        "\n",
        "# Visualize training history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Z6GQ9S3hlhlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are already defined from the previous code\n",
        "\n",
        "# Initialize the XGBoost classifier\n",
        "xgb_model = xgb.XGBClassifier(random_state=42)  # You can add hyperparameters here\n",
        "\n",
        "# Train the model\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "accuracy_percentage = accuracy * 100  # Convert accuracy to percentage\n",
        "\n",
        "# Display the accuracy in percentage\n",
        "print(f\"Accuracy: {accuracy_percentage:.2f}%\")\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=np.unique(y_resampled), yticklabels=np.unique(y_resampled))\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Y5XrQpfNlhuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-tabnet\n",
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "import torch # Importing torch\n",
        "\n",
        "# Assuming X_resampled_scaled, y_resampled are defined from your preprocessing steps\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled_scaled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
        "\n",
        "# Initialize TabNetClassifier\n",
        "clf = TabNetClassifier(\n",
        "    n_d=64,\n",
        "    n_a=64,\n",
        "    n_steps=5,\n",
        "    gamma=1.5,\n",
        "    n_independent=2,\n",
        "    n_shared=2,\n",
        "    lambda_sparse=1e-4,\n",
        "    optimizer_fn=torch.optim.Adam, # Now torch is defined and accessible\n",
        "    optimizer_params=dict(lr=2e-2),\n",
        "    mask_type=\"entmax\",\n",
        "    scheduler_params={\"gamma\": 0.95,\n",
        "                     \"step_size\": 20},\n",
        "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "    verbose=10\n",
        ")\n",
        "\n",
        "\n",
        "# Train the model\n",
        "clf.fit(\n",
        "    X_train=X_train, y_train=y_train,\n",
        "    eval_set=[(X_test, y_test)],\n",
        "    eval_name=['test'],\n",
        "    eval_metric=['accuracy'],\n",
        "    max_epochs=100,  # Adjust the number of epochs as needed\n",
        "    patience=10, # Early stopping patience\n",
        "    batch_size=1024,\n",
        "    virtual_batch_size=128,\n",
        "    num_workers=0,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2%}\")"
      ],
      "metadata": {
        "id": "xY53CWT6lhxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import FeatureUnion, Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "# ... (rest of your code for data loading, splitting, etc.) ...\n",
        "\n",
        "# Create a pipeline for the hybrid model using FeatureUnion\n",
        "# Define a function to be used with FunctionTransformer\n",
        "def decision_tree_features(X):\n",
        "    dt = DecisionTreeClassifier(random_state=42)\n",
        "    dt.fit(X, y_train)\n",
        "    return dt.apply(X).reshape(-1, 1)\n",
        "\n",
        "hybrid_model = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('features', FeatureUnion([\n",
        "        ('decision_tree', FunctionTransformer(decision_tree_features)),\n",
        "        ('original_features', 'passthrough')\n",
        "    ])),\n",
        "    ('mlp', MLPClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# Train the hybrid model\n",
        "hybrid_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = hybrid_model.predict(X_test)\n",
        "\n",
        "# Evaluation Metrics\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Optional: Display confusion matrix as a heatmap\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dkB9T14hlhzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wn7NKKL6lh2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7SSzQGyelh4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vFptwTWylh7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nvOdZs-Slh98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v4OdYRHEliAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jE1WlHmOliFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kdbrFqJyliHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cLvB2VwaliKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0btRHe95liNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lgJTcXqCliPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mji4oFHpliSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GzZloWTVliW0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}